\documentclass[a4paper,12pt,twoside,openany]{report}
%
% Wzorzec pracy dyplomowej
% J. Starzynski (jstar@iem.pw.edu.pl) na podstawie pracy dyplomowej
% mgr. inż. Błażeja Wincenciaka
% Wersja 0.1 - 8 października 2016
%
\usepackage{polski}
\usepackage{helvet}
\usepackage[T1]{fontenc}
\usepackage{anyfontsize}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{tabularx}
\usepackage{array}
\usepackage[polish]{babel}
\usepackage{subfigure}
\usepackage{amsfonts}
\usepackage{algorithmicx}
\usepackage{gnuplottex}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{verbatim}
\usepackage{indentfirst}
\usepackage[pdftex]{hyperref}
\usepackage{bm}
\usepackage{amsmath}

% rozmaite polecenia pomocnicze
% gdzie rysunki?
\graphicspath{ {rys/} }
% oznaczenie rzeczy do zrobienia/poprawienia
\newcommand{\TODO}{\textbf{TODO}}
\newcommand*\NewPage{\newpage\null\thispagestyle{empty}\newpage}



% wyroznienie slow kluczowych
\newcommand{\tech}{\texttt}

% na oprawe (1.0cm - 0.7cm)*2 = 0.6cm
% na oprawe (1.1cm - 0.7cm)*2 = 0.8cm
%  oddsidemargin lewy margines na nieparzystych stronach
% evensidemargin lewy margines na parzystych stronach
\def\oprawa{1.05cm}
\addtolength{\oddsidemargin}{\oprawa}
\addtolength{\evensidemargin}{-\oprawa}

% table span multirows
\usepackage{multirow}
\usepackage{enumitem}    % enumitem.pdf
\setlist{listparindent=\parindent, parsep=\parskip} % potrzebuje enumitem

%%%%%%%%%%%%%%% Dodatkowe Pakiety %%%%%%%%%%%%%%%%%
\usepackage{prmag2017}   % definiuje komendy opieku,nrindeksu, rodzaj pracy, ...
\usepackage{xspace}

%%%%%%%%%%%%%%% Strona Tytułowa %%%%%%%%%%%%%%%%%
% To trzeba wypelnic swoimi danymi
\title{Aplikacja do rozpoznawania emocji w sygnale mowy}

% autor
\author{Wojciech Decker}
\nrindeksu{252545}


\opiekun{dr inż. Andrzej Majkowski}
\konsultant{prof. Dzielny Konsultant}  % opcjonalnie
\terminwykonania{1 września 2017} % data na oświadczeniu o samodzielności
\rok{2017}


% Podziekowanie - opcjonalne
\podziekowania{\input{podziekowania.tex}}

% To sa domyslne wartosci
% - mozna je zmienic, jesli praca jest pisana gdzie indziej niz w ZETiIS
% - mozna je wyrzucic jesli praca jest pisana w ZETiIS
%\miasto{Warszawa}
%\uczelnia{POLITECHNIKA WARSZAWSKA}
%\wydzial{WYDZIAŁ ELEKTRYCZNY}
%\instytut{INSTYTUT ELEKTROTECHNIKI TEORETYCZNEJ\linebreak[1] I~SYSTEMÓW INFORMACYJNO-POMIAROWYCH}
\zaklad{ZAKŁAD SYSTEMÓW INFORMACYJNO POMIAROWYCH}
%\kierunekstudiow{INFORMATYKA}

% domyslnie praca jest inzynierska, ale po odkomentowaniu ponizszej linii zrobi sie magisterska
\pracamagisterska
%%% koniec od P.W

\opinie{%
	\input{opiniaopiekuna.tex}
	\newpage
	\input{recenzja.tex}
}

\streszczenia{
	\input{streszczenia.tex}
}
\newcommand{\ang}[1]{\textit{(ang. #1)}}
\newcommand{\MATLAB}{\textsc{Matlab}\xspace}

\begin{document}
\maketitle
%-----------------
% Wstęp
%-----------------
\chapter{Wstęp}
\label{ch:wstep}
% krótka definicja emocji
Emocje to stany ludzkiego umysłu.
Powstają w odpowiedzi na zdarzenie, są ukierunkowane i krótkotrwałe.
Różnią się intensywnością i zabarwieniem.
Wpływają na interpretację bodźców z otoczenia,
myśli a w konsekwencji mają istotny wpływ na zachowanie i reakcje.

% informacje zakodowane w mowie
Mowa jest nośnikiem informacji wykorzystywanym w komunikacji międzyludzkiej,
oraz pomiędzy człowiekiem i komputerem \ang{Human-Computer Interaction}.
Komunikat głosowy składa się z treści językowej,
którą można zapisać w formie tekstu,
oraz akustycznej, która również opisuje wypowiedź takie jak:
drżący ze strachu głos, czy ciężki oddech świadczący o gniewie.

% Zastosowanie maszynowego rozpoznawania emocji
Rozpoznawanie emocji mówcy jest istotne w aplikacjach wykorzystujących mowę w komunikacji człowiek-maszyna.
Zwłaszcza, jeśli odpowiedź systemu jest uzależniona od nastroju człowieka.
Klasyfikacja emocji wypowiedzi jest wykorzystywana w terapiach,
gdzie terapeuta wspiera się maszyną w odczytywaniu emocji pacjenta.
Systemy tłumaczenia maszynowego mowy mogą wykorzystać informacje mówiące o kontekście wypowiedzi i stanie mówcy.
Telefoniczne centra obsługi klienta \ang{call center} rozpoznają stan klienta,
oraz jego reakcję na ofertę prezentowaną przez konsultanta.

% Jaki jest cel?
Celem pracy jest przybliżenie tematyki komputerowego rozpoznawania mowy,
przegląd wykorzystywanych narzędzi, oraz ocena znanych rozwiązań.
Produktem końcowym będzie aplikacja komputerowa rozpoznająca emocje w sygnale mowy.
W trakcje tworzenia pracy zostanie dokonana analiza znanych rozwiązań tego zagadnienia.
Następnie zostanie opracowany schemat aplikacji, bazujący na poprzednich badaniach,
pozwalający stworzyć aplikację w warunkach tworzenia pracy magisterskiej.
Kolejnym etapem będzie implementacja aplikacji w środowisku MATLAB.
Na zakończenie zostaną przeprowadzone testy aplikacji,
analiza wyników i prezentacja wniosków.

% roboczo
Jakie współczynniki można liczyć? 
Jak przetworzyć dane, żeby otrzymać lepsze wyniki?
Jakie przetwarzanie jest wymagane?
\NewPage
\chapter{Aplikacja klasyfikująca emocje}
\section{Opis bazy danych}
\TODO{sposób katalogowania}
W pracy wykorzystano bazę danych emocji w mowie powstały w Akademii Górniczo-Hutniczej w Krakowie.
Nagrania zawierają pięć emocji podstawowych: radość, smutek, złość, strach, zdziwienie.
Ponadto zostały nagrane wypowiedzi w tonie neutralnym jako punkt odniesienia i ironicznym, jako emocja złożona.
W nagraniu wzięło udział 6 mężczyzn i 6 kobiet od 20 do 30 lat. 
Mówcy byli zarówno profesjonalnymi aktorami jak i amatorami, czy wolontariuszami.
Baza została zrealizowana jako baza danych emocji odgrywanych. 

Nagrano 4 typy wypowiedzi.
\begin{description}
	\item [Zdania] będące sekwencją 46 prostych wypowiedzi często wykorzystywanych w życiu codziennym 
		np. ,,Dzień dobry'', ,,Witam serdecznie''. 
	\item [Polecenia]  np. ,,Nowy'', ,,Otwórz''. 
		Jest to głosowa reprezentacja standardowych komend wykorzystywanych w komunikacji człowieka z komputerem.
	\item [Cyfry] od 0 do 9.
	\item [Tekst] czyli fragment artykułu. 
		Lity tekst będący spójnym logicznie ciągiem zdań, lecz wyrwanym z kontekstu.
\end{description}
Dla każdego z 6 mówców zarejestrowano każdy typ wypowiedzi we wszystkich stanach emocjonalnych.

Baza danych składa się z opisu, oraz wypowiedzi posegregowanych w katalogach oznaczających odgrywaną emocję.
Nazwy plików wskazują na mówcę, emocję oraz typ wypowiedzi.
Nagrania zostały zapisane w formie plików WAV przy częstotliwości próbkowania 44100Hz i rozdzielczości 16 bitów. ~\cite{Igras2009}.

Nagrania nie zostały poddane przetwarzaniu wstępnemu. 
Różnią się poziomem głośności.
Pojedyncze nagrania wyglądają na ustandaryzowane, ale stanowią wyjątek.
Dokładna historia nagrań nie jest znana, ich stan jest niejednolity.
Brak jest informacji o operacjach wykonywanych na nagraniach.
Stan nagrań jest niejednorodny i nieznany.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{MGR_ZL_C-plot}
	\caption{Przykład ustandaryzowanego sygnału}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{MGR_ZD_T-plot}
	\caption{Przykład nie ustandaryzowanego sygnału}
\end{figure}

Zapis wypowiedzi został ustandaryzowany w celu zniwelowania różnic wynikających z głośności nagrania.
\section{Przetwarzanie wstępne}
\TODO{
	od czytania pliku z dysku, do rozpoczęcia ekstrakcji cech

	stereo2mono, standaryzacja
}
\NewPage
\section{Ekstrakcja cech}
\subsection{Cechy bazowe}
Celem cech bazowych jest przetworzenie sygnału czasowego podzielonego na ramki,
do postaci, które zostaną przetworzone w dalszych etapach obliczeń.
Sygnał w dziedzinie czasu, ma wszystkie informacje, jakie możemy wykorzystać, 
ale analiza sygnału wyłącznie w dziedzinie czasu nie daje zadowalających rezultatów.
Przeniesienie sygnału do dziedziny częstotliwości, przez zastosowanie transformaty Fouriera,
pozwoli analizować jego zmienność w zależności od częstotliwości, 
pomijając aspekty wydatne w dziedzinie czasu.
\subsubsection{MFCC}
Jedną z najpopularniejszych cech wykorzystywanych w systemach automatycznego rozpoznawania mowy,
jest MFCC \ang{Mel-Frequency Cepstral Coefficients}.
Biorąc pod uwagę warunki powstawania mowy, cech indywidualnych mówcy, sposobu działania narządów mowy, percepcji mowy przez człowieka,
daje wektor współczynników reprezentujących mowę z założenia niezależny od mówcy. 
Przekazuje informacje o analizowanym fragmencie mowy, pomijając cechy zależne od mówiącego, takie jak częstotliwość podstawowa i jej harmoniczne.
Współczynniki MFC reprezentują dynamiczną naturę mowy, pokazując jej zmianę energii i częstotliwości w czasie.

Algorytm obliczający MFCC przyjmuje na wejście sygnał mowy o długości kilkudziesięciu ms w dziedzinie czasu, zwracając wektor współczynników \ref{rys:mgcc:schemat}.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{mfcc-schemat}
	\caption{Algorytm MFCC}
	\label{rys:mgcc:schemat}
\end{figure}

Pierwszym krokiem algorytmu jest obliczenie transformaty Fouriera. 
Pozwala ona przenieść sygnał z dziedziny czasu, do dziedziny częstotliwości. 
Operując na sygnale cyfrowym wykorzystana szybka transformata Fouriera,
bardziej precyzyjnie w wariancie szybkiej transformaty Fouriera. 
\begin{figure}[h]
	\begin{equation}
		X_{k}=\sum _{n=0}^{N-1}x_{n}e^{-i2\pi kn/N}\qquad k=0,\dots ,N-1
	\end{equation}
	\caption{Szybka transformata Fouriera}
\end{figure}
Szybka transformata Fouriera zwraca podobny wynik, jak dyskretna transformata Fouriera,
ale znacznie szybciej.

Drugim etapem algorytmu, jest reprezentacja widma w skali melowej. 
Wzorując się na ludzkim narządzie słuchu, 
który postrzega zmianę częstotliwości dźwięku nieliniowo, stosuje się skalę melową.
Skala melowa została zdefiniowana w oparciu o subiektywne odczucia wysokości dźwięku.
Za dźwięk podstawowy przyjmuje się, że dźwięk o częstotliwości 1000 Hz i głośności 40 dB wyraża się wartością 1000 melów.
Następnie na podstawie pomiarów i ocen został stworzony wzór
\begin{figure}[h]
	\begin{equation}
		m=2595\log _{10}\left(1+{\frac {f}{700}}\right)
	\end{equation}
	\caption{Wzór skali melowej}
\end{figure}
Przeniesienie widma do skali melowej wykonuje się przez aplikację szeregu trójkątnych filtrów środkowoprzepustowych. 
Wprowadza to pewną różnicę, względem psychoakustycznego postrzegania dźwięku, gdzie skala dźwięku jest ciągła.
Filtry środkowoprzepustowe wprowadzają pewną dyskretyzację możliwych wartości.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{melfilterbank}
	\caption{Środkowoprzepustowe filtry melowe}
	\label{rys:mfcc:melfilterbank}
\end{figure}
%https://srohrer32.github.io/eecs351project/

Kolejnym etapem jest obliczenie logarytmu widma w skali melowej.
Odpowiada to interpretacji głośności przez ludzkie ucho,
które odczuwa zmianę głośności w skali logarytmicznej.
\begin{figure}[h]
	\begin{equation}
		M'_k=log(M_k)\qquad k=0, \dots, N
		\label{eq:mfcc:log}
	\end{equation}
	\caption{Logarytm widma w skali melowej}
\end{figure}
Dla każdej $k$-tej wartości sygnału $M$ jest liczony logarytm, gdzie $N$ jest liczbą wartości w skali melowej \ref{eq:mfcc:log}.

Ostatnia faza to obliczenie współczynników cepstralnych.
Daje ona odseparowanie sygnału od czynników zależnych od mówcy.
Sygnał źródłowy - mowa był splotem informacji,
które przekazywał oraz sygnału zależnych od mówcy wynikających z budowy jego narządu mowy, oraz sposobu mówienia.
Cechy sygnału zależne od mówcy nie są istotne z punktu widzenia interpretacji przekazu i treści wypowiedzi.
Odfiltrowanie tych cech, pozwoli na analizę samej treści przekazu.
Cepstrum jest definiowane jako widmo widma.
Okresowy sygnał czasowy spełniający warunki Dirichleta może zostać rozwinięty w szereg Fouriera.
Rozwinięcie sygnału czasowego w trygonometryczny szereg Fouriera pozwala na analizę częstotliwości występujących w sygnale.
Odrzucając informację jak szczególnie zmieniał się sygnał, otrzymujemy informację z jakich częstotliwości sygnał się składa.
Powtarzając procedurę przenosimy widmo z dziedziny częstotliwości, do $quefrency$, opisującej zależności pomiędzy częstotliwościami,
pomijając informację wartości tych częstotliwości. 
W rezultacie uniezależniamy się od częstotliwości podstawowej i formant, które są ściśle zależne od mówcy,
a otrzymujemy informację w jaki sposób te częstotliwości się zmieniały, niezależnie od ich wartości.
Z definicji przeniesienie 

\TODO zrozumieć

\subsubsection{Energia}
Energia sygnału jest sumą kwadratów próbek w ramce.
\begin{figure}[h]
	\begin{equation}
		E_{s}=\sum _{n=-\infty }^{\infty }{|x(n)|^{2}}
		\label{eq:energy}
	\end{equation}
	\caption{Energia sygnału}
\end{figure}
Energia sygnału jest wprost proporcjonalna do głośności wypowiedzi.
\subsubsection{Liczba przejść przez zero}
Liczba przejść przez zero \ang{zero crossing rate} znajduje zastosowanie w systemach detekcji mowy.
Pozwala ona stwierdzić, czy w danej ramce występuje mowa.
Jest też zależna od częstotliwości tonu. 
Liczbę przejść przez zero wyraża się w liczbie miejsc, gdzie sąsiednie próbki sygnału zmieniły wartość z dodatniej na ujemną lub z ujemnej na dodatnią.
\begin{figure}[h]
	\begin{equation}
		Z = \sum_{n=2}^{N}s(x_n \cdot x_{n-1})\text{, gdzie } s(x) = 
		\begin{cases} 
			0 \text{, dla } x \geq 0 \\
			1 \text{, dla } x < 0
		\end{cases}
		\label{eq:zcr}
	\end{equation}
	\caption{Liczba przejść przez zero}
\end{figure}
Jeżeli liczba przejść przez zero jest duża, można założyć, że dana ramka nie zawiera mowy.
Ramki zawierające mowę mają wysoką liczbę przejść przez zero. \cite{Walters-Williams2010}
\subsubsection{Energia entropii}
Entropia energii mówi o sumie prawdopodobieństw wystąpienia danej wartości próbki $p_k$ w sygnale.
\begin{figure}[h]
	\begin{equation}
		Ee_{s}=-\sum _{k=1}^{N}p_k log(p_k)
		\label{eq:entropy_energy}
	\end{equation}
	\caption{Energia entropii}
\end{figure}\cite{Majstorovic2011}
Jeżeli w ramce występuje wiele skrajnych wartości, entropia sygnału będzie duża. 
Pozwala to stwierdzić zmienność i rozbieżność wartości sygnału w ramce.
\subsubsection{SSC}
Centroidy Podpasm Widma \ang{Spectral Subband Centroids} opisuje cechy widma podobne do formant.
Analizując widmo sygnału dzieli je na podpasma, następnie znajdując środek ciężkości w każdym podpaśmie.
\begin{figure}[h]
	\begin{equation}
		C_m=\frac
		{\int_{l_m}^{h_m} f w_m(f) P^\gamma(f)df}
		{\int_{l_m}^{h_m} w_m(f) P^\gamma(f)df}
		\label{eq:ssc}
	\end{equation}
	\caption{SSC}
\end{figure}
Powyższa formułą definiuje centroidy, gdzie dzieląc widmo na $m$ pasm:
\begin{description}
	\item[$C_m$] $m$-ty centroid
	\item[$l_m$] dolna granica pasma
	\item[$h_m$] górna granica pasma
	\item[$w_m$] funkcja filtru
	\item[$P(f)$] moc widma
	\item[$\gamma$] współczynnik mocy widma
\end{description}
SSC pozwala na znaczną parametryzację, przez określenie szerokości pasma, np połowa częstotliwości próbkowania sygnału.\cite{Majstorovic2011}
Kolejną decyzją jest określenie kształtu filtra środkowoprzepustowego, definiującego pasmo, 
jak i liczba, oraz wzajemne położenie pasm.
\subsection{Opis statystyczny}
Ostatnim etapem ekstrakcji cech, jest określenie ich parametrów statystycznych.
Dla każdej ramki sygnału zostały obliczone cechy bazowe, opisane w poprzedniej sekcji.
Zostaną one zbadane statystycznie tworząc właściwe cechy, na podstawie których zostanie dokonana klasyfikacja.
\begin{enumerate}
	\item minimum 
	\item maksimum 
	\item średnia 
	\item wariancja 
	\item skośność
	\item kurioza
\end{description}
\TODO{Opisać  'minmax', 'mean', 'variance', 'skewness', 'kurtosis'}
\NewPage
\NewPage
\NewPage
\section{Selekcja cech}
\subsection{SFS}
\NewPage
\NewPage
\NewPage
\subsection{PCA}
\NewPage
\NewPage
\NewPage
\section{Klasyfikacja}
\subsection{Klasyfikator Najbliższych Sąsiadów}
Klasyfikator najbliższych sąsiadów pozwala szybko otrzymać rezultaty. 
Jest prosty w działaniu i implementacji.
Otrzymane rezultaty mogą nie być zadowalające w bardziej złożonych zagadnieniach.
Zasada działania w uproszczeniu sprowadza się do reprezentacji obserwacji za pomocą wektorów cech.
Obliczane są odległości pomiędzy wektorami. \cite{Du2013}
Klasa jest przypisywana na podstawie klasy najbliższej, sąsiedniej obserwacji.
Klasyfikator jest podatny na ilość cech branych pod uwagę w czasie klasyfikacji.
Redukcja liczby parametrów, uwzględniająca usunięcie cech silnie skorelowanych i słabo różnicujących znacząco podnosi wydajność i skuteczność klasyfikatora.

Metoda klasyfikuje obserwacje biorąc pod uwagę najbliższą obserwację ze zbioru uczącego.
Obserwacje ze zbioru uczącego rozmieszczone są w wielowymiarowej przestrzeni cech.
Przestrzeń ta jest podzielona na strefy odpowiadające klasom.
Obserwacja znajdująca się w danej strefie zostaje przypisana do klasy do której ta strefa należy.
Podział na strefy odbywa się w czasie fazy uczenia klasyfikatora.
Kształt i klasa strefy zależy od odległości obserwacji ze zbioru uczącego.
Punkt w przestrzeni przypisany zostaje do klasy, jeżeli w jego najbliższym sąsiedztwie zostanie znalezionych $k$ sąsiadów należących do tej klasy.
Klasyfikatorowi należy zdefiniować liczbę $k$ - liczbę wymaganych sąsiadów klasy, pozwalającą określić klasę badanej obserwacji, 
oraz funkcję liczącą dystans pomiędzy dwoma wektorami. Najczęściej jest to norma euklidesowa. \cite{Martin2011}

\begin{algorithm}
	\caption{Klasyfikator Najbliższych sąsiadów}
	\begin{algorithmic}[1]
		\Procedure {kNN}{$T$, $k$, $dist$, $o$}
		\State $D \leftarrow []$
		\ForAll {$t \in T$}
		\State $D(t) \leftarrow dist(t, o)$
		\EndFor
		\State $S \leftarrow sort(D, T)$ \Comment obserwacje posortowane względem odległości
		\State $selectedClass \leftarrow 0$
		\State $counter \leftarrow []$ \Comment licznik najbliższych sąsiadów z klasy
		\Repeat
		\ForAll {$s \in S$}
		\State $counter(class(s))++$
		\EndFor
		\Until{$\neg selectedClass$}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Dane wejściowe muszą być znormalizowane. 
W przeciwnym razie cechy o bezwzględnie wyższych wartościach będą miały większy wpływ na klasyfikację,
bez rozróżnienia ich zdolności różnicowania zbioru.
Algorytm jest wrażliwy na liczbę cech - wymiarów przestrzeni w której umieszczana jest obserwacja.
Każda cecha wprowadza wymiar równoważny z pozostałymi wymiarami. 
Nie jest brana pod uwagę korelacja cech, ani ich zdolność różnicowania obserwacji.
Dla klasyfikowanej obserwacji obliczany jest dystans pomiędzy nią,
a każdą obserwacją ze zbiory testowego, uwzględniająca wszystkie wymiary.
Prowadzi to do znacznego wzrostu złożoności obliczeniowej klasyfikacji, dla większej liczby cech, czy zbiorów danych uczących.

Istnieją modyfikacje klasyfikatora najbliższych sąsiadów redukujące wymienione wady. 
Opisana została klasyczna wersja algorytmu.

\subsection{Maszyny Wektorów Wspierających}

Maszyny Wektorów Wspierających \ang{Support Vector Machine} wykorzystywany jest do klasyfikacji obserwacji w uczeniu maszynowym.
Należy do grupy modeli uczonych w sposób nadzorowany. 
Uczenie odbywa się przez zadanie zbioru danych testowych, 
każda obserwacja jest oznaczona jako należąca do jednej z dwóch klas.
Algorytm uczący SVM tworzy model rozróżniający obserwacje obydwu klas.
Jako rezultat powstaje klasyfikator binarny, nie probabilistyczny. 

Model powstaje przez rozmieszczenie obserwacji w przestrzeni wielowymiarowej.
Każda obserwacja zostaje umieszczona w $n$-wymiarowej przestrzeni cech i reprezentowana jest przez wektor
zaczynający się w początku układu współrzędnych, a kończący się w punkcie określającym obserwację. 
Hiperpłaszczyzna \ang{hyperplane} jest $n-1$ wymiarowa i zgodnie z definicją rozdziela przestrzeń na dwie części.
Poszukiwana hiperpłaszczyzna powinna być hiperpłaszczyzną optymalną \ang{optimal hyperplane}, 
to czyli znajdować się w jak największej odległości od najbliższych obserwacji należących do różnych klas.
\cite{Cortes1995}

Biorąc przestrzeń dwuwymiarową, możemy ją rozmieścić na płaszczyźnie. 
Na płaszczyźnie tej kładziemy dowolną prostą. 
Prosta ta dzieli płaszczyznę na dwie części.
Prosta jest więc hiperpłaszczyzną przestrzeni dwuwymiarowej.
Analogicznie sytuacja przestawia się w przestrzeni trójwymiarowej.
Może ją przeciąć dowolna płaszczyzna.
Płaszczyzna dzieli przestrzeń trójwymiarową na dwie części, jest więc jej hiperpłaszczyzną.

Sposób tworzenia klasyfikatora SVM przedstawia się następująco.
Obserwacje $x_i$ rozmieszczone są w $n$-wymiarowej przestrzeni cech, gdzie $x_i \in R^n$.
Klasy $y_i$ są oznaczone jako $-1$ i $+1$, gdzie $y_i \in \{-1, +1\}$, co można zapisać jako:
\begin{equation}
	(\bm{x_1},y_1), ..., (\bm{x_i}, y_i), \textrm{ gdzie } y_i \in \{-1, +1\}
\end{equation}

Obserwacje są liniowo separowalne, jeżeli istnieje wektor $\bm{w}$ i taki skalar $b$, że nierówności
\begin{gather}
	\bm{w} \cdot \bm{x_i} + b \geq 1 \textrm{ jeśli } y_i = 1\\
	\bm{w} \cdot \bm{x_i} + b \leq 1 \textrm{ jeśli } y_i = -1,
\end{gather}
są prawdziwe dla wszystkich obserwacji w zbiorze.
\cite{Cortes1995}
Powyższy układ nierówności można zapisać w postaci:
\begin{equation}
	y_i(\bm{w} \cdot \bm{x_i} + b) \geq 1
\end{equation}
Hiperpłaszczyzna optymalna 
\begin{equation}
	\bm{w_0} \cdot \bm{x_i}  + b_0 = 0
\end{equation}

istnieje w przypadku, kiedy rozdziela zbiór treningowy z zachowaniem największej możliwej odległości od obserwacji obydwu klas.
Odległość pomiędzy obserwacjami a hiperpłaszczyzną definiuje się następująco:
\begin{equation}
	\rho(\bm{w}, b) = \min\limits_{\{x:y=1\}} \frac{\bm{x} \cdot \bm{w}} {|\bm{w}|} - \max\limits_{\{x:y=-1\}}  \frac{\bm{x} \cdot \bm{w}} {|\bm{w}|} 
\end{equation}
czyli rzut skalarny \ang{scalar projection} wektora obserwacji $\bm{x}$ na wektor jednostkowy $\hat{\bm{w}} = \frac{\bm{w}}{|\bm{w}|}$
obrazujący położenie płaszczyzny.
Wartość rzutu skalarnego jest odległością obserwacji od hiperpłaszczyzny. 
Różnica najmniejszej odległości pomiędzy hiperpłaszczyzną, a obserwacją należącą do klasy $y=1$,
oraz największej odległości pomiędzy hiperpłaszczyzną, a obserwacją należącą do klasy $y=-1$ 
powinna być jak największa.
Obserwacje znajdujące się w najmniejszej odległości od hiperpłaszczyzny nazywane są wektorami wspierającymi \ang{support vectors}

Klasyfikacja polega na umieszczeniu klasyfikowanej obserwacji w przestrzeni cech.
Obserwacja należy do tej klasy, do której należą obserwacje leżące po tej samej stronie hiperpłaszczyzny.
\subsubsection{Uczenie modelu}
%http://fourier.eng.hmc.edu/e161/lectures/svm/node3.html
Uczenie odbywa się przez zadanie zbioru obserwacji testowych. 
Mamy zbiór obserwacji, separowalny liniowo, każda z obserwacji należy do klasy $P$ lub $N$:
\begin{displaymath}
	\{ ({\bf x}_k, y_k), k=1,\cdots,K \} 
\end{displaymath}
gdzie klasy są oznaczone jako $y_k \in \{1,-1\}$ określają obserwacje $\vec{x}_k$.
Celem optymalizacji jest odnalezienie wektora $\vec{w}$ i stałej $b$ wyznaczających hiperpłaszczyznę
sperarującą liniowo obserwacje różnych klas w przestrzeni. 

Weźmy wektor początkowy $\vec{w} = 0$ i zbiór danych uczących $K$.
Obliczamy pierwszą wartość wektora $\vec{w}$
\begin{equation}\label{eq:svm:hyperplane}
	{\vec{w}}=\sum_{i=1}^K \alpha_i y_i {\vec{x}}_i
\end{equation}
gdzie $\alpha_i>0$. 
Podczas każdego cyklu uczącego, wektor $\vec{w}$ może zostać zmodyfikowany przez obserwację, 
zgodnie z regułą:
\begin{equation}
	\mbox{jeżeli } y_i f({\vec{x}}_i)=y_i ({\vec{x}}_i^T{\vec{w}}^{stary}+b)
	=y_i\left(\sum_{j=1}^m \alpha_j y_j({\vec{x}}_i^T{\vec{x}}_j)+b\right)<0,
\end{equation}
\begin{equation}
	\mbox{to } {\vec{w}}^{nowy}={\vec{w}}^{stary}+\eta y_i {\vec{x}}_i
	=\sum_{j=1}^{m} \alpha_j y_j\vec{x}_j +\eta y_i {\vec{x}}_i,
\end{equation}
\begin{equation}
	\mbox{np. }
	\alpha_i^{nowy}=\alpha_i^{stary}+\eta
\end{equation}
gdzie $0 < \eta < 1$ jest parametrem uczenia.
Ostatecznie płaszczyznę \ref{eq:svm:hyperplane} określamy dobierając wartości $\alpha_i$ według zależności: 
\begin{equation}
	\mbox{jeżeli } y_i\left(\sum_{j=1}^m \alpha_j y_j({\vec{x}}_i^T\vec{x}_j)+b\right)<0,
	\mbox{to } \alpha_i^{new}=\alpha_i^{old}+\eta 
\end{equation}

\subsubsection{Hiperpłaszczyzna Miękkiego Marginesu}
Rzeczywiste problemy rozmieszczone w przestrzeni wielowymiarowej nie są separowalne liniowo. 
Rozwiązanie problemu nieseparowalnego liniowo sprowadza się do znalezienia hiperpłaszczyzny Miękkiego Marginesu \ang{Soft Margin}, 
która rozdziela obserwacje dwóch klas zawierającą jak najmniejszą liczbę błędnych klasyfikacji.
W takim przypadku błędne klasyfikacje są kosztem nie do uniknięcia. 
Poszukiwanie rozwiązania polega na wyznaczeniu hiperpłaszczyzny rozdzielającej przestrzeń 
minimalizując błędy klasyfikacji na zbiorze uczącym i nieznanym zbiorze testowym.

Przyjmijmy zbiór uczący $D$, który nie jest separowalny liniowo. 
Nie istnieje hiperpłaszczyzna rozdzielająca obserwacje dwóch klas.
Zostanie więc stworzony margines błędu, dopuszczający istnienie błędnych klasyfikacji w swoim obrębie.
Występowanie błędnych klasyfikacji jest tolerowane, ale wystąpienia błędnych klasyfikacji są minimalizowane.
Błędną klasyfikację wyraża się niepewnością $\xi_i$ \ang{slack variable}. 
Wartość niepewności $\xi_i$ jest wprost proporcjonalna do odległości obserwacji $\vec{x_i}$
Formalnie problem optymalizacji SVM można zapisać w postaci:

\begin{equation}
\mbox{zminimalizuj } {\vec w}^T {\vec w}+C\sum_{i=1}^m \xi_i^k
\end{equation}
%http://fourier.eng.hmc.edu/e161/lectures/svm/node2.html

\begin{equation}
	\mbox{zakładając, że }  y_i ({\bf x}_i^T {\bf w}+b) \ge 1-\xi_i,
	\mbox{, }x_i \ge 0 \mbox{, }(i=1,\cdots,m)
\end{equation}
Parametr $C$ steruje maksymalizacją marginesu i minimalizacją błędów uczenia.
Jeżeli parametr $C$ ma małą wartość, margines dopuszcza zbyt wiele błędów klasyfikacji.
Duża wartość parametru $C$ może skutkować przeuczeniem modelu.
Parametr $k$ określa normę marginesu. 
Margines pierwszej normy jest mniej wrażliwy na obserwacje leżące poza nim,
dzięki temu margines pierwszej normy jest lepszy do klasyfikacji danych mniej uporządkowanych.

\subsubsection{Funkcje jądrowe}
Funkcja jądrowa \ang{kernel method} 
przyporządkowuje każdej reprezentacji obserwacji w przestrzeni,
reprezentację w innej przestrzeni o większej liczbie wymiarów.
Zabieg zwiększenia liczby wymiarów ma na celu ułatwienie 
klasyfikacji, czy klasteryzacji zbioru danych względem
reprezentacji w przestrzeni pierwotnej. 

Przykładem jest SVM, poszukujący hiperpłaszczyzny.
Jeżeli dane nie są seprowalne liniowo,
ale są separowalne nieliniowo to stosując sztuczkę jądrową \ang{kernel trick} 
przeniesiemy zbiór obserwacji do przestrzeni w której będzie on separowalny liniowo.
\cite{Patle2013}

\begin{equation}
	\langle \vec{x_1} \cdot \vec{x_2} \rangle \gets K(\vec{x_1}, \vec{x_2}) = \langle \Phi(\vec{x_1}) \cdot \Phi(\vec{x_2}) \rangle
\end{equation}

Jądro liniowe \ref{eq:svm:kern:lin} jest najprostszą funkcją jądrową. 
sprowadza się do iloczyny wektorowego i dodania stałej $c$.
Liniowa funkcja jądrowa w niektórych przypadkach nie wprowadza zmian zmieniających rezultat działania algorytmu.
\begin{figure}[hc]
	\begin{equation}
		\label{eq:svm:kern:lin}
		K(x, x_i) = x \cdot x ^ T + c
	\end{equation}
	\caption{Liniowa funkcja jądrowa}
\end{figure}

Jądro wielomianowe \ref{eq:svm:kern:poly} znajduje zastosowanie przy problemach,
gdzie dane są znormalizowane. 
Wielomianową funkcję jądrową określają współczynnik $\alpha$, stałą $c$ i stopień wielomiany $d$.
\begin{figure}[hc]
	\begin{equation}
		\label{eq:svm:kern:poly}
		K(x, x_i) = (\alpha x^T x_i + c)^d 
	\end{equation}
	\caption{Wielomianowa funkcja jądrowa}
\end{figure}

Jądro sigmoidalne \ref{eq:svm:kern:sig} pochodzi z dziedziny sieci neuronowych,
gdzie funkcjonuje sigmoidalna funkcja aktywacji neuronu. 
Model SVM z sigmoidalną funkcją jądrową jest równoważny dwuwarstwowej perceptonowej sieci neuronowej.
Posiada dwa parametry: współczynnik $\alpha$ i stałą $c$, gdzie zazwyczaj $c$ przyjmuje wartość $1/N$,
dla $N$-wymiarowej przestrzeni.
\begin{figure}[hc]
	\begin{equation}
		\label{eq:svm:kern:sig}
		K(x_i, x_j) = tanh( \alpha x_i  x_j + c)
	\end{equation}
	\caption{Sigmoidalna funkcja jądrowa}
\end{figure}
%http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/
\subsection{MLP}
\subsubsection{Sztuczne sieci neuronowe}
Sztuczne sieci neuronowe \ang{Artificial Neural Network} są modelami zainspirowanymi ludzkim mózgiem.
Składają się z ogromnej liczby węzłów, każdy węzeł wykonuje prostą operację matematyczną.
Każdy neuron posiada wejścia, wyjście, oraz zestaw parametrów go określających. 
Dzięki połączeniom węzłów - sztucznych neuronów sieci neuronowe są w stanie rozwiązywać złożone problemy.
Proces uczenia sieci neuronowej polega na dostosowywaniu parametrów węzłów prowadzącego do znalezienia optymalnego rozwiązania problemu.
Sztuczne sieci neuronowe są bardzo popularnym i rozbudowanym zagadnieniem uczenia maszynowego.
Znajdują zastosowanie w rozpoznawaniu obrazów, mowy, pisma, robotyce. 

\subsubsection{Model neuronu}
Ludzki mózg jest złożony z ok $10^{11}$ neuronów.
Odbierają one sygnały od innych neuronów, do których są podłączone.
Na podstawie wejścia decydują, czy podać sygnał na wejście.
Pojedynczy neuron nie rozwiązuje złożonych problemów.
Wiele neuronów połączonych w sieć jest w stanie wykonać skomplikowane operacje matematyczne.
Pojedynczy neuron składa się z wejścia $\vec{x}$, wyjścia $y$, wektora wag $\vec{w}$ i skoku $b$.
Decyzję o podaniu sygnału na wyjście podejmuje funkcja aktywacji $g(\vec{x} \cdot \vec{w} + b)$.
Wyjściem neuronu jest $0$, $1$, lub liczba zbliżona do którejś z tych wartości.
Na potrzeby tej sekcji przyjmijmy progową funkcję aktywacji $g(x)$ \ref{eq:mlp:step}.
\begin{figure}[hc]
	\begin{equation}
		\label{eq:mlp:step}
	g(x)={\begin{cases}0{\text{, dla }}x<0\\1{\text{, dla }}x\geq 0\end{cases}}
	\end{equation}
	\caption{Progowa funkcja aktywacji}
\end{figure}
Neutron przyjmuje na wejście wektor $\vec{x}$.
Wektor wejściowy zostaje pomnożony przez wektor wag $\vec{w}$ i dodany do progu aktywacji $b$.
Rezultat jest argumentem funkcji aktywacji  $g(\vec{x} \cdot \vec{w} + b)$.
Wartość funkcji aktywacji zostaje podana na wyjście. \ref{rys:mlp:neuron}
Możliwości pojedynczego neuronu są ograniczone. 
Posiada binarne wyjście $0$ lub $1$, 
Zadaniem neuronu jest określenie, po której stronie hiperpłaszczyzny, 
określonej przez wektor $\vec w$ i skalar $b$.
Funkcja aktywacji normalizuje odpowiedź.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{neuron}
	\caption{Schemat neuronu}
	\label{rys:mlp:neuron}
\end{figure}

\subsubsection{Funkcja aktywacji}
Funkcja aktywacji określa odpowiedź neuronu.
Co do zasady neuron zwraca odpowiedź binarną \textit{wszystko albo nic}.
Przykładem jest skokowa funkcja aktywacji \ref{eq:mlp:step}.
\begin{figure}[h]
	\label{wyk:mlp:step}
	\centering
	\begin{gnuplot}[terminal=pdf,terminaloptions=color]
		set xrange [-1:1] 
		set yrange [-.5:1.5]
		step(x) = x>0 ? 1 : 0
		plot step(x)
	\end{gnuplot}
	\begin{equation}
		step(x)={\begin{cases}0{\text{, dla }}x<0\\1{\text{, dla }}x\geq 0\end{cases}}
	\end{equation}
	\caption{Progowa funkcja aktywacji}
\end{figure}

W praktyce stosuje się funkcje ciągłe, dające wartość z zakresu od $0$ do $1$.
Najczęściej spotykaną ciągłą funkcją logiczną jest funkcja sigmoidalna.
Może przyjąć wszystkie wartości z zakresu $y \in (0,1)$.
Rzutuje to na interpretację wartości zwracanej przez neuron,
która nie działa już zgodnie z regułą \textit{wszystko, albo nic}.
Neuron z funkcją sigmoidalną nie jest już klasyfikatorem binarnym, ponieważ może zwrócić np. wartość $sigm(0) = 0,5$.
Pozwala to na tworzenie bardziej ogólnych sieci neuronowych, znajdujących zastosowanie w większej liczbie zastosowań.

%http://lowrank.net/gnuplot/datafile2-e.html

\begin{figure}[h]
	\label{wyk:mlp:sigm}
	\centering
	\begin{gnuplot}[terminal=pdf,terminaloptions=color]
		set xrange [-10:10] 
		set yrange [-.5:1.5]
		sigm(x) = 1/(1+exp(-x))
		plot sigm(x)
	\end{gnuplot}
	\begin{equation}
		sigm(x)=\frac{1}{1+e^{-x}}
	\end{equation}
	\caption{Sigmoidalna funkcja aktywacji}
\end{figure}
\subsubsection{Schemat i zasada działania sztucznej sieci neuronowej}
Sieć neuronowa zbudowana jest z wielu neuronów połączonych ze sobą.
Wyjścia jednych neuronów przekazywane są jako wejścia dla kolejnych.
Schemat budowy sieci neuronowej można przedstawić za pomocą grafu skierowanego \ref{rys:mlp:graf}.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{ann-graf}
	\caption{Sieć neuronowa jako graf}
	\label{rys:mlp:graf}
\end{figure}
Węzły grafu oznaczają neurony, a krawędzie wagi wejść neuronów do których prowadzą.
Wyjścia jednych neuronów są wejściami dla drugich.
Wyróżniamy dwa typy sieci neuronowych.
Sieci neuronowe jednokierunkowe, w reprezentacji grafowej tworzą acykliczny graf skierowany.
Dzięki wyeliminowaniu cykli z sieci jej reprezentacja, uczenie i obliczenia zostają w znacznej mierze uproszczone. 
Sieci neuronowe w których występują cykle, nazywane są sieciami rekurencyjnymi.
Węzły $x_1$ i $x_2$ są wejściami sieci neuronowej.
Węzły $s_{1..5}$, $y_1$ i $y_2$ są neuronami z funkcją aktywacji.
Wartości wyjściowe z węzłów $y_1$ i $y_2$ są wyjściami całej sieci neuronowej.
Przykładowo wejście $x_1$ zostaje pomnożone przez wagę $w_{x_1s_1}$ i staje się pierwszym wejściem dla neuronu $s_1$, itd.

Stosuje się logiczny podział węzłów na warstwy \ref{rys:mlp:warstwy}.
Warstwa $L_0$ nazywana jest warstwą wejściową.
Przyjmuje ona wektor $\vec x$, który jest przetwarzany w kolejnych warstwach.
Warstwy $L_1$ i $L_2$ są warstwami ukrytymi sieci. 
Wartości przyjmowane i zwracane przez neurony warstw ukrytych nie są widoczne z zewnątrz sieci.
W sieciach jednokierunkowych każda warstwa zależy wyłącznie od wartości z warstwy poprzedniej.
Wartości zwrócone przez warstwę $L_3$ - wyjściową są wynikiem działania całej sieci neuronowej.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{ann-warstwy}
	\caption{Warstwy sieci neuronowej}
	\label{rys:mlp:warstwy}
\end{figure}
\subsubsection{Percepton wielowarstwowy}
Percepton wielowarstwowy \ang{Multi-layer percepton} jest najpopularniejszym typem sztucznej sieci neuronowej.
Jest zbudowany z warstw wejściowej, wyjściowej i wielu warstw ukrytych.
Percepton wielowarstwowy w odróżnieniu od perceptonu jednowarstwowego może rozwiązywać problemy nieliniowe.
Przykładem funkcji, której nie może zamodelować pojedynczy percepton jest XOR \ang{exclusive or} \ref{tab:mlp:xor}.
\begin{figure}[h]
	\centering
	\begin{tabular}{ |l|c|r| }
		\hline
		$x_1$	& $x_2$& $XOR$ \\ \hline
		0 	& 0 	& 0 \\ \hline
		0 	& 1 	& 1 \\ \hline
		1 	& 0 	& 1 \\ \hline
		1 	& 1 	& 0 \\ \hline
	\end{tabular}
	\caption{Tabela funkcji XOR}
	\label{tab:mlp:xor}
\end{figure}
Wykres \ref{wyk:mlp:xor} obrazuje funkcję $XOR$, oraz przykładowe hiperpłaszczyzny.
Nie istnieje hiperpłaszczyzna rozdzielająca argumenty, dla których wartość $XOR$ przyjmuje $1$,
od argumentów dla których $XOR$ przyjmuje wartość $0$.
\begin{figure}[h]
	\label{wyk:mlp:sigm}
	\centering
	\begin{gnuplot}[terminal=pdf,terminaloptions=color]
		set xrange [-1:2]
		set yrange [-1:2]
		linia1(x)=x+.5
		linia2(x)=-2*x+1.5
		plot "-" using 1:2 title 'XOR(x1, x2)=1',\
		"-" using 1:2  title 'XOR(x1, x2)=0',\
		linia1(x),\
		linia2(x)
		0	1
		1	0
		end
		0	0
		1	1
		end
	\end{gnuplot}
	\caption{Wykres XOR}
	\label{wyk:mlp:xor}
\end{figure}
Percepton wielowarstwowy dzięki zastosowaniu wielu warstw ukrytych, 
jest w stanie zróżnicować zbiory, które nie są separowalne liniowo.
\chapter{Wyniki}
\TODO{
	Wywołania aplikacji.
	Różne klasyfikatory, parametry, klasy.
	Dużo tabel.
}
\begin{figure}[hc!]
	\centering
	\input{./app/accuaricy_table.tex}
	\caption{Otrzymane wyniki}
\end{figure}

\NewPage
\NewPage
\NewPage
\NewPage
\NewPage
\NewPage
\NewPage

\chapter{Wnioski}
\NewPage
\NewPage

\bibliography{praca_dyplomowa}{}
\bibliographystyle{plain}
\end{document}
%+++ END +++
