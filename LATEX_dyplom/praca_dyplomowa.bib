@article{Gurgen2017,
author = {G{\"{u}}rgen, Samet and {\"{U}}nver, Bedir and Altın, İsmail},
doi = {10.1016/j.renene.2017.10.101},
file = {:tmp/1-s2.0-S0960148117310716-main.pdf:pdf},
issn = {09601481},
journal = {Renewable Energy},
pages = {538--544},
title = {{Prediction of cyclic variability in a diesel engine fueled with n-butanol and diesel fuel blends using artificial neural network}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0960148117310716},
volume = {117},
year = {2017}
}
@article{Jeon2018,
abstract = {{\textcopyright} 2017 Objectives To assess and compare predictive factors for persistent hemodynamic depression (PHD) after carotid artery angioplasty and stenting (CAS) using artificial neural network (ANN) and multiple logistic regression (MLR) or support vector machines (SVM) models. Patients and methods A retrospective data set of patients (n = 76) who underwent CAS from 2007 to 2014 was used as input (training cohort) to a back-propagation ANN using TensorFlow platform. PHD was defined when systolic blood pressure was less than 90 mm Hg or heart rate was less 50 beats/min that lasted for more than one hour. The resulting ANN was prospectively tested in 33 patients (test cohort) and compared with MLR or SVM models according to accuracy and receiver operating characteristics (ROC) curve analysis. Results No significant difference in baseline characteristics between the training cohort and the test cohort was observed. PHD was observed in 21 (27.6{\%}) patients in the training cohort and 10 (30.3{\%}) patients in the test cohort. In the training cohort, the accuracy of ANN for the prediction of PHD was 98.7{\%} and the area under the ROC curve (AUROC) was 0.961. In the test cohort, the number of correctly classified instances was 32 (97.0{\%}) using the ANN model. In contrast, the accuracy rate of MLR or SVM model was both 75.8{\%}. ANN (AUROC: 0.950; 95{\%} CI [confidence interval]: 0.813–0.996) showed superior predictive performance compared to MLR model (AUROC: 0.796; 95{\%} CI: 0.620–0.915, p  {\textless}  0.001) or SVM model (AUROC: 0.885; 95{\%} CI: 0.725-0.969, p  {\textless}  0.001). Conclusions The ANN model seems to have more powerful prediction capabilities than MLR or SVM model for persistent hemodynamic depression after CAS. External validation with a large cohort is needed to confirm our results.},
author = {Jeon, J.P. and Kim, C. and Oh, B.-D. and Kim, S.J. and Kim, Y.-S.},
doi = {10.1016/j.clineuro.2017.12.005},
file = {:tmp/1-s2.0-S0303846717303414-main.pdf:pdf},
issn = {18726968},
journal = {Clinical Neurology and Neurosurgery},
keywords = {Artificial neural network,Carotid artery,Stenting},
number = {December 2017},
pages = {127--131},
publisher = {Elsevier},
title = {{Prediction of persistent hemodynamic depression after carotid angioplasty and stenting using artificial neural network model}},
url = {https://doi.org/10.1016/j.clineuro.2017.12.005},
volume = {164},
year = {2018}
}
@article{Paliwal1998,
author = {Paliwal, Kuldip K},
file = {:tmp/00675340.pdf:pdf},
pages = {7--10},
title = {{Sub {\~{}} id features for speech recognit1}},
year = {1998}
}
@article{Labib2010,
author = {Labib, Richard and Khattar, Karim},
doi = {10.1007/s00521-009-0309-4},
file = {:tmp/10.1007{\%}2Fs00521-009-0309-4.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Bilinear boundary,Explicit equation,Hidden space,Parameter optimization,Three-layered perceptron},
number = {2},
pages = {305--315},
title = {{MLP bilinear separation}},
volume = {19},
year = {2010}
}
@article{Majstorovic2011,
author = {Majstorovic, Nemanj and Andric, Milenko and Mikluc, Davorin},
file = {:tmp/06143635.pdf:pdf},
isbn = {9781457715006},
keywords = {- entropy-based,energy-based,recognition},
number = {4},
pages = {667--670},
title = {{Entropy-based algorithm for speech recognition in noisy environment}},
year = {2011}
}
@book{Walters-Williams2010,
author = {Walters-Williams, Janett and Li, Yan},
booktitle = {Advanced Techniques in Computing Sciences and Software Engineering},
doi = {10.1007/978-90-481-3660-5-14},
file = {:tmp/10.1007{\%}2F978-90-481-3660-5.pdf:pdf},
isbn = {9789048136599},
keywords = {Euclidean distance,Hamming distance,Kullback-leibler distance,Mahalanobis distance,Manhattan distance,Minkowski distance,Nearest neighbor},
pages = {79--84},
title = {{Comparative study of distance functions for nearest neighbors}},
year = {2010}
}
@article{Hastie2001,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
archivePrefix = {arXiv},
arxivId = {1010.3003},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
doi = {10.1198/jasa.2004.s339},
eprint = {1010.3003},
file = {:home/wdk/Dokumenty/ESLII.pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {03436993},
journal = {The Mathematical Intelligencer},
keywords = {inger series in statistics},
number = {2},
pages = {83--85},
pmid = {21196786},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf{\%}255Cnhttp://www-stat.stanford.edu/{~}tibs/book/preface.ps},
volume = {27},
year = {2001}
}
@article{Demystified2010,
abstract = {After a couple of disastrous experiments trying to teach EM, we carefully wrote this tutorial to give you an intuitive and mathematically rigorous understanding of EM and why it works. We explain the standard applications of EM to learning Gaussian mixture models (GMMs) and hidden Markov models (HMMs), and prepare you to apply EM to new problems. This tutorial assumes you have an advanced undergraduate understanding of probability and statistics.},
author = {Demystified, E M and Tutorial, An Expectation-maximization and Chen, Yihua and Gupta, Maya R},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Demystified et al. - 2010 - EM Demystified An Expectation-Maximization Tutorial.pdf:pdf},
journal = {Electrical Engineering},
number = {206},
pages = {1--17},
title = {{EM Demystified: An Expectation-Maximization Tutorial}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:EM+Demystified:+An+Expectation-Maximization+Tutorial{\#}0},
year = {2010}
}
@article{Kuaga2011,
abstract = {Growth references are useful in monitoring a child's growth, which is an essential part of child care. The aim of this paper was to provide updated growth references for Polish school-aged children and adolescents and show the prevalence of overweight and obesity among them. Growth references for height, weight, and body mass index (BMI) were constructed with the lambda, mu, sigma (LMS) method using data from a recent, large, population-representative sample of school-aged children and adolescents in Poland (n = 17,573). The prevalence of overweight and obesity according to the International Obesity Taskforce definition was determined with the use of LMSGrowth software. Updated growth references for Polish school-aged children and adolescents were compared with Polish growth references from the 1980s, the Warsaw 1996-1999 reference, German, and 2000 CDC references. A positive secular trend in height was observed in children and adolescents from 7 to 15 years of age. A significant shift of the upper tail of the BMI distribution occurred, especially in Polish boys at younger ages. The prevalence of overweight or obesity was 18.7{\%} and 14.1{\%} in school-aged boys and girls, respectively. The presented height, weight, and BMI references are based on a current, nationally representative sample of Polish children and adolescents without known disorders affecting growth. Changes in the body size of children and adolescents over the last three decades suggest an influence of the changing economical situation on anthropometric indices.},
author = {Ku{\l}aga, Zbigniew and Litwin, Mieczys{\l}aw and Tkaczyk, Marcin and Palczewska, Iwona and Zaja̧czkowska, Ma{\l}gorzata and Zwoli{\'{n}}ska, Danuta and Krynicki, Tomasz and Wasilewska, Anna and Moczulska, Anna and Morawiec-Knysak, Aurelia and Barwicka, Katarzyna and Grajda, Aneta and Gurzkowska, Beata and Napieralska, Ewelina and Pan, Huiqi},
doi = {10.1007/s00431-010-1329-x},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ku{\l}aga et al. - 2011 - Polish 2010 growth references for school-aged children and adolescents.pdf:pdf},
isbn = {0340-6199},
issn = {03406199},
journal = {European Journal of Pediatrics},
keywords = {Adolescents,Children,Growth references,LMS method},
number = {5},
pages = {599--609},
pmid = {20972688},
title = {{Polish 2010 growth references for school-aged children and adolescents}},
volume = {170},
year = {2011}
}
@article{Cortes1995,
abstract = {High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1023/A:1022627411411},
eprint = {arXiv:1011.1669v3},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortes, Vapnik - 1995 - Support-vector networks.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
number = {3},
pages = {273--297},
pmid = {19549084},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@article{You2010a,
abstract = {Among conventional methods for text-independent speaker recognition, Gaussian mixture model (GMM) is known for its effectiveness and scalability in modeling the spectral distribution of speech. A GMM-supervector characterizes a speaker's voice by the GMM parameters such as the mean vectors, covariance matrices and mixture weights. Besides the first-order statistics, it is generally believed that speaker's cues are partly conveyed by the second-order statistics. In this paper, we introduce a Bhattacharyya-based GMM-distance to measure the distance between two GMM distributions. Subsequently, the GMM-UBM mean interval (GUMI) concept is introduced to derive a GUMI kernel which can be used in conjunction with support vector machine (SVM) for speaker recognition. The GUMI kernel allows us to exploit the speaker's information not only from the mean vectors of GMM but also from the covariance matrices. Moreover, by analyzing the Bhattacharyya-based GMM-distance measure, we extend the Bhattacharyya-based kernel by involving both the mean and covariance statistical dissimilarities. We demonstrate the effectiveness of the new kernel on the National Institute of Standards and Technology (NIST) speaker recognition evaluation (SRE) 2006 dataset.},
author = {You, Chang Huai and Lee, Kong Aik and Li, Haizhou},
doi = {10.1109/TASL.2009.2032950},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/You, Lee, Li - 2010 - GMM-SVM kernel with a Bhattacharyya-based distance for speaker recognition.pdf:pdf},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Gaussian mixture model (GMM),speaker recognition,supervector,support vector machine (SVM)},
number = {6},
pages = {1300--1312},
title = {{GMM-SVM kernel with a Bhattacharyya-based distance for speaker recognition}},
volume = {18},
year = {2010}
}
@article{Seehapoch2013,
abstract = {Automatic recognition of emotional states from human speech is a current research topic with a wide range. In this paper an attempt has been made to recognize and classify the speech emotion from three language databases, namely, Berlin, Japan and Thai emotion databases. Speech features consisting of Fundamental Frequency (F0), Energy, Zero Crossing Rate (ZCR), Linear Predictive Coding (LPC) and Mel Frequency Cepstral Coefficient (MFCC) from short-time wavelet signals are comprehensively investigated. In this regard, Support Vector Machines (SVM) is utilized as the classification model. Empirical experimentation shows that the combined features of F0, Energy and MFCC provide the highest accuracy on all databases provided using the linear kernel. It gives 89.80{\%}, 93.57{\%} and 98.00{\%} classification accuracy for Berlin, Japan and Thai emotions databases, respectively.},
author = {Seehapoch, Thapanee and Wongthanavasu, Sartra},
doi = {10.1109/KST.2013.6512793},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Seehapoch, Wongthanavasu - 2013 - Speech Emotion Recognition Using Support Vector Machines.pdf:pdf},
isbn = {9781467348539},
journal = {2013 5th International Conference on Knowledge and Smart Technology (KST)},
keywords = {-component,speech emotion recognitions,support},
pages = {86--91},
title = {{Speech Emotion Recognition Using Support Vector Machines}},
url = {http://ieeexplore.ieee.org/document/6512793/},
year = {2013}
}
@article{Patle2013,
abstract = {A new generation learning system based on recent advances in statistical learning theory deliver state-of-the-art performance in real-world applications that is Support Vector Machines [2]. Applications such as text categorization, hand-written character recognition, image classification, bio-sequence analysis [9] etc for the classification and regression Most of the existing supervised classification methods are based on traditional statistics, which can provide ideal results when sample size is tending to infinity. However, only finite samples can be acquired in practice. In this paper, a novel learning method, Support Vector Machine (SVM), is applied on different data. This paper emphasizes the classification task with Support Vector Machine with different kernel function. It has several kernel functions including linear, polynomial and radial basis for performing classification [13]. {\textcopyright} 2013 IEEE.},
author = {Patle, Arti and Chouhan, Deepak Singh},
doi = {10.1109/ICAdTE.2013.6524743},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Patle, Chouhan - 2013 - SVM kernel functions for classification.pdf:pdf},
isbn = {9781467356183},
journal = {2013 International Conference on Advances in Technology and Engineering, ICATE 2013},
keywords = {Kernel,feature,radial basis function,support vector},
title = {{SVM kernel functions for classification}},
year = {2013}
}
@article{Chen2012,
author = {Chen, Yanxiang and Xie, Jian},
doi = {10.1007/s11767-012-0871-2},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Xie - 2012 - Emotional speech recognition based on SVM with GMM supervector.pdf:pdf},
issn = {0217-9822},
journal = {Journal of Electronics (China)},
keywords = {emotional speech recognition,gaussian mixture,gmm,model,supervector,support vector machines,svm,universal background model,usb},
number = {3-4},
pages = {339--344},
title = {{Emotional speech recognition based on SVM with GMM supervector}},
url = {http://link.springer.com/10.1007/s11767-012-0871-2},
volume = {29},
year = {2012}
}
@article{Sannakki2013,
author = {Sannakki, Sanjeev S and Rajpurohit, Vijay S and Nargund, V B},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sannakki, Rajpurohit, Nargund - 2013 - SVM-DSD SVM Based Diagnostic System.pdf:pdf},
keywords = {color transformation,fea-,k-means clustering,plant pathology,support vector machine,svm,ture extraction},
number = {Figure 1},
pages = {715--720},
title = {{SVM-DSD : SVM Based Diagnostic System}},
year = {2013}
}
@article{Frihia2017,
author = {Frihia, Hamza and Bahi, Halima},
doi = {10.1007/s10772-017-9427-z},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frihia, Bahi - 2017 - HMMSVM segmentation and labelling of Arabic speech for speech recognition applications.pdf:pdf},
isbn = {0123456789},
issn = {15728110},
journal = {International Journal of Speech Technology},
keywords = {Arabic language,HMM,SVM,Speech recognition,Speech segmentation},
number = {3},
pages = {563--573},
publisher = {Springer US},
title = {{HMM/SVM segmentation and labelling of Arabic speech for speech recognition applications}},
volume = {20},
year = {2017}
}
@article{July2011,
author = {July, Washington},
doi = {10.1007/978-0-387-73003-5_225},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/July - 2011 - Sample Quality.pdf:pdf},
isbn = {978-0-387-73002-8},
journal = {Ratio},
pages = {1--11},
title = {{Sample Quality}},
year = {2011}
}
@article{Roddick2009,
author = {Roddick, John F},
doi = {10.1007/978-0-387-39940-9_1532},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Roddick - 2009 - Schema Evolution {\&} Schema Versioning.pdf:pdf},
isbn = {978-0-387-39940-9},
journal = {Encyclopedia of Database Systems},
pages = {2479--2481, 2499--2502},
title = {{Schema Evolution $\backslash${\&} Schema Versioning}},
url = {http://dx.doi.org/10.1007/978-0-387-39940-9{\_}1532},
year = {2009}
}
@article{Mittal2016,
author = {Mittal, Teena and Sharma, R. K.},
doi = {10.1007/s11771-016-3191-0},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mittal, Sharma - 2016 - Integrated search technique for parameter determination of SVM for speech recognition.pdf:pdf},
isbn = {1177101631910},
issn = {22275223},
journal = {Journal of Central South University},
keywords = {Hooke-Jeeves method,Mel-frequency cepstral coefficients,predator prey optimization,speech recognition,support vector machine (SVM),wavelet packets},
number = {6},
pages = {1390--1398},
title = {{Integrated search technique for parameter determination of SVM for speech recognition}},
volume = {23},
year = {2016}
}
@book{Mukherjee2002,
author = {Mukherjee, Neelanjan and Mukherjee, Sayan},
booktitle = {Pattern Recognition with Support Vector Machines},
doi = {10.1007/3-540-45665-1_1},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mukherjee, Mukherjee - 2002 - Predicting Signal Peptides with Support Vector Machines.pdf:pdf},
isbn = {978-3-540-44016-1},
pages = {1--7},
title = {{Predicting Signal Peptides with Support Vector Machines}},
url = {http://www.ulb.tu-darmstadt.de/tocs/79304567.pdf{\%}5Cnhttp://link.springer.com/content/pdf/10.1007/3-540-45665-1.pdf{\%}5Cnhttp://link.springer.com/10.1007/3-540-45665-1{\_}1{\%}5Cnhttp://link.springer.com/10.1007/3-540-45665-1},
volume = {2388},
year = {2002}
}
@article{Yang2007,
author = {Yang, Xiao-yuan and Liu, Jia and Zhang, Min-qing and Niu, Ke},
doi = {10.1007/978-3-540-72588-6_114},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2007 - A New Multi-class SVM Algorithm Based on One-Class SVM.pdf:pdf},
isbn = {9783540725879},
issn = {03029743},
journal = {Computational Science–ICCS 2007},
keywords = {machine learning,multi-class svm,one-class svm},
pages = {677--684},
title = {{A New Multi-class SVM Algorithm Based on One-Class SVM}},
year = {2007}
}
@article{Redei2008,
abstract = {e occupants of neighboring fields have trouble understanding each others' papers”{\ldots} “The language used in Reports and Research Articles is sufficiently technical and arcane that they are hard to understand, even for those in related disciplines.” (Kennedy, D 2007 Science 318:715). One of the most fascinating features of science is its continuous evolution. My goal was to emphasize the principles, provide numerical data and guide to resources that are more difficult to access from other publications. The book can assist the reader to make better use of the Internet but the Internet and textbooks are no substitutes for this book. Unlike the majority of books, this Encyclopedia will not be outdated because the continuously renewed and updated databases and web sites listed assure its usefulness even in the distant future. Describing individual genes in different organisms—with some exceptions—is no longer practical in a single book or even with the aid of an excellent resource such as PubMed due to the multitude of abstracts (more than 15,000,000 entries from more than 19,000 journals). The web sites—listed in this book—can however, provide great help in identifying many genes, their synonyms, functions and interacting partners as well as critical references to them. Although basic statistical concepts are explained in simple terms, most of the theoretical mathematical models or detailed laboratory procedures are not included because of the difficulties of describing the techniques within the limited space available in a single book. The abundance of references can lead the reader in the right direction. Selection of papers for inclusion is also a continuous challenge. During 1992 to 2001 4,061 journals published more than 3.47 million peer-reviewed articles in health-related areas alone (Paraje, G et al. 2005 Science 308:959). Although this book is quite complex, integrated, and referenced, it does not include everything but it might be a useful guide to almost everything one needs in biology. For a proof of principle, I suggest that you look up any concept what you know or what you are uncertain or have doubts about},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {R{\'{e}}dei, George P.},
doi = {10.1016/B978-0-12-374984-0.01419-4},
eprint = {arXiv:1011.1669v3},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/R{\'{e}}dei - 2008 - Syntax.pdf:pdf},
isbn = {9780123749840},
issn = {1098-6596},
journal = {Encyclopedia of Genetics, Genomics, Proteomics and Informatics},
pages = {1912},
pmid = {25246403},
title = {{Syntax}},
url = {http://link.springer.com/referenceworkentry/10.1007/978-1-4020-6754-9{\_}16534{\#}},
year = {2008}
}
@article{Schwenker2009,
abstract = {Emotion recognition from speech is an important field of research in human-machine-interfaces, and has various applications, for instance for call centers. In the proposed classifier system RASTA-PLP features (perceptual linear prediction) are extracted from the speech signals. The first step is to compute an universal background model (UBM) representing a general structure of the underlying feature space of speech signals. This UBM is modeled as a Gaussian mixture model (GMM). After computing the UBM the sequence of feature vectors extracted from the utterance is used to re-train the UBM. From this GMM the mean vectors are extracted and concatenated to the so-called GMM supervectors which are then applied to a support vector machine classifier. The overall system has been evaluated by using utterances from the public Berlin emotional database. Utilizing the proposed features a recognition rate of 79{\%} (utterance based) has been achieved which is close to the performance of humans on this database.},
author = {Schwenker, Friedhelm and Scherer, Stefan and Magdi, Yasmine M. and Palm, G{\"{u}}nther},
doi = {10.1007/978-3-642-04274-4_92},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwenker et al. - 2009 - The GMM-SVM supervector approach for the recognition of the emotional status from speech.pdf:pdf},
isbn = {3642042732},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {894--903},
title = {{The GMM-SVM supervector approach for the recognition of the emotional status from speech}},
volume = {5768 LNCS},
year = {2009}
}
@article{Kukharchik2008,
abstract = {This paper investigates the adaptation of modified wavelet-based features and support vector machines for vocal folds pathology detection. A new type of feature vector, based on continuous wavelet transform of input audio data is proposed for this task. Support vec-tor machine was used as a classifier for testing the feature extraction procedure. The results of the experimental study are shown.},
author = {Kukharchik, P. and Kheidorov, I. and Bovbel, E. and Ladeev, D.},
doi = {10.1007/978-3-540-69905-7_22},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kukharchik et al. - 2008 - Speech signal processing based on wavelets and SVM for vocal tract pathology detection.pdf:pdf},
isbn = {978-3-540-69905-7},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {192--199},
title = {{Speech signal processing based on wavelets and SVM for vocal tract pathology detection}},
volume = {5099 LNCS},
year = {2008}
}
@article{Ding2015,
abstract = {Speech applications, which operate a system by voice commands, facilitate web access for disabled and visually impaired users. Human-computer interactions, such as speaking and listening to web applications, provide options for developing a multimodal interaction tool in the accessible design of an intelligent web. Speaker identification and verification are essential functionalities for intelligent web programs with speech applications. This paper proposes an enhanced Gaussian mixture model (GMM) method by incorporating the information derived from the support vector machine (SVM), called EGMM-SVM, for web-based applications with speaker recognition. The EGMM-SVM improves the accuracy of the estimated likelihood scores between the speech frame and the GMM. In EGMM-SVM, SVM plays a crucial role in transmitting the quality information of the utterances from a test speaker, through the GMM when performing GMM likelihood calculations. The experimental results show that speaker recognition by using the developed EGMM-SVM with an accurate operation mechanism for Gaussian distribution derivations yields a higher recognition rate than does a conventional GMM without any considerations on the quality of test speech utterances. {\textcopyright} 2013, Springer Science+Business Media New York.},
author = {Ding, Ing Jr and Yen, Chih Ta},
doi = {10.1007/s11042-013-1587-5},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Yen - 2015 - Enhancing GMM speaker identification by incorporating SVM speaker verification for intelligent web-based speech appli.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {EGMM-SVM,GMM likelihood score,Gaussian mixture model,Speaker recognition,Support vector machine},
number = {14},
pages = {5131--5140},
title = {{Enhancing GMM speaker identification by incorporating SVM speaker verification for intelligent web-based speech applications}},
volume = {74},
year = {2015}
}
@article{You2010,
abstract = {Among conventional methods for text-independent speaker recognition, Gaussian mixture model (GMM) is known for its effectiveness and scalability in modeling the spectral distribution of speech. A GMM-supervector characterizes a speaker's voice by the GMM parameters such as the mean vectors, covariance matrices and mixture weights. Besides the first-order statistics, it is generally believed that speaker's cues are partly conveyed by the second-order statistics. In this paper, we introduce a Bhattacharyya-based GMM-distance to measure the distance between two GMM distributions. Subsequently, the GMM-UBM mean interval (GUMI) concept is introduced to derive a GUMI kernel which can be used in conjunction with support vector machine (SVM) for speaker recognition. The GUMI kernel allows us to exploit the speaker's information not only from the mean vectors of GMM but also from the covariance matrices. Moreover, by analyzing the Bhattacharyya-based GMM-distance measure, we extend the Bhattacharyya-based kernel by involving both the mean and covariance statistical dissimilarities. We demonstrate the effectiveness of the new kernel on the National Institute of Standards and Technology (NIST) speaker recognition evaluation (SRE) 2006 dataset.},
author = {You, Chang Huai and Lee, Kong Aik and Li, Haizhou},
doi = {10.1109/TASL.2009.2032950},
file = {:tmp/05256328.pdf:pdf},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Gaussian mixture model (GMM),speaker recognition,supervector,support vector machine (SVM)},
number = {6},
pages = {1300--1312},
title = {{GMM-SVM kernel with a Bhattacharyya-based distance for speaker recognition}},
volume = {18},
year = {2010}
}
@article{Elangovan2011,
abstract = {The studies on tool condition monitoring along with digital signal processing can be used to prevent damages on cutting tools and workpieces when the tool conditions become faulty. These studies have become more relevant in today's context where the order realization dates are crunched and deadlines are to be met in order to catch up with the competition. Based on a continuous acquisition of signals with sensor systems it is possible to classify certain wear parameters by the extraction of features. Data mining approach is extensively used to probe into structural health of the tool and the process. This paper discusses condition monitoring of carbide tipped tool using Support Vector Machine and compares the classification efficiency between C-SVC and $\nu$-SVC. It further analyses the results with other classifiers like Decision Tree and Na{\"{i}}ve Bayes and Bayes Net. The vibration signals are acquired for various tool conditions like tool-good condition, tip-breakage, etc. The effort is to bring out the better features-classifier combination. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {Elangovan, M. and Sugumaran, V. and Ramachandran, K. I. and Ravikumar, S.},
doi = {10.1016/j.eswa.2011.05.081},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elangovan et al. - 2011 - Effect of SVM kernel functions on classification of vibration signals of a single point cutting tool.pdf:pdf},
isbn = {09574174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Decision Tree,Feature extraction,Statistical features,Support Vector Machine,Tool condition monitoring},
number = {12},
pages = {15202--15207},
publisher = {Elsevier Ltd},
title = {{Effect of SVM kernel functions on classification of vibration signals of a single point cutting tool}},
url = {http://dx.doi.org/10.1016/j.eswa.2011.05.081},
volume = {38},
year = {2011}
}
@article{Fletez-Brant2013,
abstract = {Massively parallel sequencing technologies have made the generation of genomic data sets a routine component of many biological investigations. For example, Chromatin immunoprecipitation followed by sequence assays detect genomic regions bound (directly or indirectly) by specific factors, and DNase-seq identifies regions of open chromatin. A major bottleneck in the interpretation of these data is the identification of the underlying DNA sequence code that defines, and ultimately facilitates prediction of, these transcription factor (TF) bound or open chromatin regions. We have recently developed a novel computational methodology, which uses a support vector machine (SVM) with kmer sequence features (kmer-SVM) to identify predictive combinations of short transcription factor-binding sites, which determine the tissue specificity of these genomic assays (Lee, Karchin and Beer, Discriminative prediction of mammalian enhancers from DNA sequence. Genome Res. 2011; 21:2167-80). This regulatory information can (i) give confidence in genomic experiments by recovering previously known binding sites, and (ii) reveal novel sequence features for subsequent experimental testing of cooperative mechanisms. Here, we describe the development and implementation of a web server to allow the broader research community to independently apply our kmer-SVM to analyze and interpret their genomic datasets. We analyze five recently published data sets and demonstrate how this tool identifies accessory factors and repressive sequence elements. kmer-SVM is available at http://kmersvm.beerlab.org.},
author = {Fletez-Brant, Christopher and Lee, Dongwon and McCallion, Andrew S. and Beer, Michael A.},
doi = {10.1093/nar/gkt519},
file = {:tmp/gkt519.pdf:pdf},
isbn = {1362-4962},
issn = {13624962},
journal = {Nucleic acids research},
number = {Web Server issue},
pages = {544--556},
pmid = {23771147},
title = {{kmer-SVM: a web server for identifying predictive regulatory sequence features in genomic data sets.}},
volume = {41},
year = {2013}
}
@article{Huang2017,
abstract = {Breast cancer is an all too common disease in women, making how to effectively predict it an active research problem. A number of statistical and machine learning techniques have been employed to develop various breast cancer prediction models. Among them, support vector machines (SVM) have been shown to outperform many related techniques. To construct the SVM classifier, it is first necessary to decide the kernel function, and different kernel functions can result in different prediction performance. However, there have been very few studies focused on examining the prediction performances of SVM based on different kernel functions. Moreover, it is unknown whether SVM classifier ensembles which have been proposed to improve the performance of single classifiers can outperform single SVM classifiers in terms of breast cancer prediction. Therefore, the aim of this paper is to fully assess the prediction performance of SVM and SVM ensembles over small and large scale breast cancer datasets. The classification accuracy, ROC, F-measure, and computational times of training SVM and SVM ensembles are compared. The experimental results show that linear kernel based SVM ensembles based on the bagging method and RBF kernel based SVM ensembles with the boosting method can be the better choices for a small scale dataset, where feature selection should be performed in the data pre-processing stage. For a large scale dataset, RBF kernel based SVM ensembles based on boosting perform better than the other classifiers.},
author = {Huang, Min Wei and Chen, Chih Wen and Lin, Wei Chao and Ke, Shih Wen and Tsai, Chih Fong},
doi = {10.1371/journal.pone.0161501},
file = {:tmp/pone.0161501.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {1},
pages = {1--14},
pmid = {28060807},
title = {{SVM and SVM ensembles in breast cancer prediction}},
volume = {12},
year = {2017}
}
@article{GarciaMolina2014,
abstract = {Robust detection of prostatic cancer is a challenge due to the multitude of variants and their representation in MR images. We propose a pattern recognition system with an incremental learning ensemble algorithm using support vector machines (SVM) tackling this problem employing multimodal MR images and a texture-based information strategy. The proposed system integrates anatomic, texture, and functional features. The data set was preprocessed using B-Spline interpolation, bias field correction and intensity standardization. First- and second-order angular independent statistical approaches and rotation invariant local phase quantization (RI-LPQ) were utilized to quantify texture information. An incremental learning ensemble SVM was implemented to suit working conditions in medical applications and to improve effectiveness and robustness of the system. The probability estimation of cancer structures was calculated using SVM and the corresponding optimization was carried out with a heuristic method together with a 3-fold cross-validation methodology. We achieved an average sensitivity of 0.844 ± 0.068 and a specificity of 0.780 ± 0.038, which yielded superior or similar performance to current state of the art using a total database of only 41 slices from twelve patients with histological confirmed information, including cancerous, unhealthy non-cancerous and healthy prostate tissue. Our results show the feasibility of an ensemble SVM being able to learn additional information from new data while preserving previously acquired knowledge and preventing unlearning. The use of texture descriptors provides more salient discriminative patterns than the functional information used. Furthermore, the system improves selection of information, efficiency and robustness of the classification. The generated probability map enables radiologists to have a lower variability in diagnosis, decrease false negative rates and reduce the time to recognize and delineate structures in the prostate.},
author = {{Garc{\'{i}}a Molina}, Jos{\'{e}} Fernando and Zheng, Lei and Sertdemir, Metin and Dinter, Dietmar J. and Schon̈berg, Stefan and Rad̈le, Matthias},
doi = {10.1371/journal.pone.0093600},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garc{\'{i}}a Molina et al. - 2014 - Incremental learning with SVM for multimodal classification of prostatic adenocarcinoma.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {4},
pmid = {24699716},
title = {{Incremental learning with SVM for multimodal classification of prostatic adenocarcinoma}},
volume = {9},
year = {2014}
}
@article{Abdulkadir2011,
abstract = {Fully automated machine learning methods based on structural magnetic resonance imaging (MRI) data can assist radiologists in the diagnosis of Alzheimer's disease (AD). These algorithms require large data sets to learn the separation of subjects with and without AD. Training and test data may come from heterogeneous hardware settings, which can potentially affect the performance of disease classification.A total of 518 MRI sessions from 226 healthy controls and 191 individuals with probable AD from the multicenter Alzheimer's Disease Neuroimaging Initiative (ADNI) were used to investigate whether grouping data by acquisition hardware (i.e. vendor, field strength, coil system) is beneficial for the performance of a support vector machine (SVM) classifier, compared to the case where data from different hardware is mixed. We compared the change of the SVM decision value resulting from (a) changes in hardware against the effect of disease and (b) changes resulting simply from rescanning the same subject on the same machine.Maximum accuracy of 87{\%} was obtained with a training set of all 417 subjects. Classifiers trained with 95 subjects in each diagnostic group and acquired with heterogeneous scanner settings had an empirical detection accuracy of 84.2 ?? 2.4{\%} when tested on an independent set of the same size. These results mirror the accuracy reported in recent studies. Encouragingly, classifiers trained on images acquired with homogenous and heterogeneous hardware settings had equivalent cross-validation performances. Two scans of the same subject acquired on the same machine had very similar decision values and were generally classified into the same group. Higher variation was introduced when two acquisitions of the same subject were performed on two scanners with different field strengths. The variation was unbiased and similar for both diagnostic groups.The findings of the study encourage the pooling of data from different sites to increase the number of training samples and thereby improving performance of disease classifiers. Although small, a change in hardware could lead to a change of the decision value and thus diagnostic grouping. The findings of this study provide estimators for diagnostic accuracy of an automated disease diagnosis method involving scans acquired with different sets of hardware. Furthermore, we show that the level of confidence in the performance estimation significantly depends on the size of the training sample, and hence should be taken into account in a clinical setting. ?? 2011 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Abdulkadir, Ahmed and Mortamet, B{\'{e}}n{\'{e}}dicte and Vemuri, Prashanthi and Jack, Clifford R. and Krueger, Gunnar and Kl{\"{o}}ppel, Stefan},
doi = {10.1016/j.neuroimage.2011.06.029},
eprint = {NIHMS150003},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abdulkadir et al. - 2011 - Effects of hardware heterogeneity on the performance of SVM Alzheimer's disease classifier.pdf:pdf},
isbn = {1095-9572 (Electronic) 1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
keywords = {Alzheimer's disease,MRI,Magnetic resonance imaging,Multi-site study,Support vector machines (SVM)},
number = {3},
pages = {785--792},
pmid = {21708272},
publisher = {Elsevier Inc.},
title = {{Effects of hardware heterogeneity on the performance of SVM Alzheimer's disease classifier}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2011.06.029},
volume = {58},
year = {2011}
}
@article{Dellepiane2015,
abstract = {The Standard Quadratic Problem (StQP) is an NP-hard problem with many local minimizers (stationary points). In the literature, heuristics based on unconstrained continuous non-convex formulations have been proposed (Bomze {\&} Palagi, 2005; Bomze, Grippo, {\&} Palagi, 2012) but none dominates the other in terms of best value found. Following (Cassioli, DiLorenzo, Locatelli, Schoen, {\&} Sciandrone, 2012) we propose to use Support Vector Machines (SVMs) to define a multistart global strategy which selects the "best" heuristic. We test our method on StQP arising from the Maximum Clique Problem on a graph which is a challenging combinatorial problem. We use as benchmark the clique problems in the DIMACS challenge.},
author = {Dellepiane, Umberto and Palagi, Laura},
doi = {10.1016/j.ejor.2014.09.054},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dellepiane, Palagi - 2015 - Using SVM to combine global heuristics for the Standard Quadratic Problem.pdf:pdf},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Clique,Data mining,Global optimization,Maximum,Nonlinear programming,Problem,Quadratic programming},
number = {3},
pages = {596--605},
publisher = {Elsevier Ltd.},
title = {{Using SVM to combine global heuristics for the Standard Quadratic Problem}},
url = {http://dx.doi.org/10.1016/j.ejor.2014.09.054},
volume = {241},
year = {2015}
}
@article{Teoretycznej2017,
author = {Teoretycznej, Instytut Elektrotechniki},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Teoretycznej - 2017 - Wojciech Decker(2).pdf:pdf},
title = {{Wojciech Decker}},
year = {2017}
}
@article{Martin2011,
abstract = {The aim of this paper is to present (jointly) a series of robust high performance (award winning) implementations of reinforcement learning algorithms based on temporal-difference learning and weighted k- nearest neighbors for linear function approximation. These algorithms, named kNNTD(;) methods, where rigorously tested at the Second and Third Annual Reinforcement Learning Competitions (RLC2008 and RCL2009) held in Helsinki and Montreal respectively, where the kNNTD(;) method (JAMH team) won in the PolyAthlon 2008 domain, obtained the second place in 2009 and also the second place in the Mountain-Car 2008 domain showing that it is one of the state of the art general purpose reinforcement learning implementations. These algorithms are able to learn quickly, to generalize properly over continuous state spaces and also to be robust to a high degree of environmental noise. Furthermore, we describe a derivation of kNNTD(;) algorithm for problems where the use of continuous actions have clear advantages over the use of fine grained discrete actions: the Ex〈a〉 reinforcement learning algorithm. {\textcopyright} 2010 Elsevier B.V.},
author = {{Mart{\'{i}}n H}, Jos{\'{e}} Antonio and de Lope, Javier and Maravall, Dar{\'{i}}o},
doi = {10.1016/j.neucom.2010.07.027},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mart{\'{i}}n H, de Lope, Maravall - 2011 - Robust high performance reinforcement learning through weighted k-nearest neighbors.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Function approximation,K-Nearest-neighbors,Reinforcement learning,Temporal-difference learning},
number = {8},
pages = {1251--1259},
title = {{Robust high performance reinforcement learning through weighted k-nearest neighbors}},
volume = {74},
year = {2011}
}
@article{Du2013,
abstract = {Text categorization is a significant technique to manage the surging text data on the Internet. The k-nearest neighbors (kNN) algorithm is an effective, but not efficient, classification model for text categorization. In this paper, we propose an effec-tive strategy to accelerate the standard kNN, based on a simple principle: usually, near points in space are also near when they are projected into a direction, which means that distant points in the projection direction are also distant in the original space. Using the proposed strategy, most of the irrelevant points can be removed when searching for the k-nearest neighbors of a query point, which greatly decreases the computation cost. Experimental results show that the proposed strategy greatly improves the time per-formance of the standard kNN, with little degradation in accuracy. Specifically, it is superior in applications that have large and high-dimensional datasets.},
author = {Du, Min and Chen, Xing-Shu},
doi = {10.1631/jzus.C1200303},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Du, Chen - 2013 - Accelerated k-nearest neighbors algorithm based on principal component analysis for text categorization.pdf:pdf},
issn = {1869-1951},
journal = {J Zhejiang Univ-Sci C (Comput {\&} Electron)},
keywords = {Accelerating strategy,Principal component analysis (PCA),Text categorization,k-nearest neighbors (kNN)},
number = {6},
pages = {407--416},
title = {{Accelerated k-nearest neighbors algorithm based on principal component analysis for text categorization *}},
url = {https://link.springer.com/content/pdf/10.1631{\%}2Fjzus.C1200303.pdf},
volume = {14},
year = {2013}
}
@article{Nielsena2016,
abstract = {Modelling signals as being periodic is common in many applications. Such periodic signals can be represented by a weighted sum of sinusoids with frequencies being an integer multiple of the fundamental frequency. Due to its widespread use, numerous methods have been proposed to estimate the fundamental frequency, and the maximum likelihood (ML) estimator is the most accurate estimator in statistical terms. When the noise is assumed to be white and Gaussian, the ML estimator is identical to the non-linear least squares (NLS) estimator. Despite being optimal in a statistical sense, the NLS estimator has a high computational complexity. In this paper, we propose an algorithm for lowering this complexity significantly by showing that the NLS estimator can be computed efficiently by solving two Toeplitz-plus-Hankel systems of equations and by exploiting the recursive-in-order matrix structures of these systems. Specifically, the proposed algorithm reduces the time complexity to the same order as that of the popular harmonic summation method which is an approximate NLS estimator. The performance of the proposed algorithm is assessed via Monte Carlo and timing studies. These show that the proposed algorithm speeds up the evaluation of the NLS estimator by a factor of 50–100 for typical scenarios.},
author = {Nielsen, Jesper Kj{\ae}r and Jensen, Tobias Lindstr{\o}m and Jensen, Jesper Rindom and Christensen, Mads Gr{\ae}sb{\o}ll and Jensen, S{\o}ren Holdt},
doi = {10.1016/j.sigpro.2017.01.011},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nielsen et al. - 2017 - Fast fundamental frequency estimation Making a statistically efficient estimator computationally efficient.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Fast algorithms,Fundamental frequency estimation,Hankel,Pitch,Toeplitz},
number = {October 2016},
pages = {188--197},
title = {{Fast fundamental frequency estimation: Making a statistically efficient estimator computationally efficient}},
volume = {135},
year = {2017}
}
@article{Boersma1993,
abstract = {We present a straightforward and robust algorithm for periodicity detection, working in the lag (autocorrelation) domain. When it is tested for periodic signals and for signals with additive noise or jitter, it proves to be several orders of magnitude more accurate than the methods commonly used for speech analysis. This makes our method capable of measuring harmonics-to-noise ratios in the lag domain with an accuracy and reliability much greater than that of any of the usual frequency-domain methods.},
author = {Boersma, Paul},
doi = {10.1371/journal.pone.0069107},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boersma - 1993 - Accurate Short-Term Analysis of the Fundamental Frequency and the Harmonics-To-Noise Ratio of a Sampled Sound.pdf:pdf},
journal = {Proceedings of the Institute of Phonetic Sciences},
pages = {97--110},
title = {{Accurate Short-Term Analysis of the Fundamental Frequency and the Harmonics-To-Noise Ratio of a Sampled Sound}},
url = {http://www.cs.northwestern.edu/{~}pardo/courses/eecs352/papers/boersma-pitchtracking.pdf},
volume = {17},
year = {1993}
}
@article{KrishnaKishore2013,
abstract = {Recognition of emotions from speech is one of the most important sub domains in the field of affective computing. Six basic emotional states are considered for classification of emotions from speech in this work. In this work, features are extracted from audio characteristics of emotional speech by Mel-frequency Cepstral Coefficient (MFCC), and Subband based Cepstral Parameter (SBC) method. Further these features are classified using Gaussian Mixture Model (GMM). SAVEE audio database is used in this work for testing of Emotions. In the experimental results, SBC method out performs with 70{\%} in recognition compared to 51{\%} of recognition in MFCC algorithm.},
author = {{Krishna Kishore}, K. V. and {Krishna Satish}, P.},
doi = {10.1109/IAdCC.2013.6514336},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krishna Kishore, Krishna Satish - 2013 - Emotion recognition in speech using MFCC and wavelet features.pdf:pdf},
isbn = {9781467345286},
journal = {Proceedings of the 2013 3rd IEEE International Advance Computing Conference, IACC 2013},
keywords = {Gaussian Mixture Model (GMM),Mel-frequency Cepstral Coefficient (MFCC),Subband based Cepstral Parameter (SBC)},
pages = {842--847},
title = {{Emotion recognition in speech using MFCC and wavelet features}},
year = {2013}
}
@article{Igras2009,
abstract = {Streszczenie. Artyku{\l} prezentuje opracowan{\c{a}} w AGH baz{\c{e}} danych nagra{\'{n}} mowy emocjonalnej zgromadzonej w celu bada{\'{n}} nad zawarto{\'{s}}ci{\c{a}} afektywn{\c{a}} sygna{\l}u mowy. Opisano spos{\'{o}}b rejestracji, parametry, struktur{\c{e}}, metadane i licencj{\c{e}} bazy danych. Przedstawiono przyk{\l}adowe zastosowania do opracowania metod detekcji stan{\'{o}}w emocjonalnych w g{\l}osie oraz normalizacji nagra{\'{n}} na potrzeby ASR. S{\l}owa kluczowe: nagrania mowy, detekcja emocji w g{\l}osie Summary. The paper presents a database of emotional speech recordings collected in AGH for research on affective content of speech signal. We describe the method of data acquisition, the parameters, structure, metadata and license. We present example applications for development of the methods of emotions detection in voice and emotional speech normalization for ASR.},
author = {Igras, Magdalena and {ZI{\'{O}}{\L}KO AGH Akademia G{\'{o}}rniczo-Hutnicza im Stanis{\l}awa Staszica Krakowie}, Bartosz and Elektroniki, Katedra},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Igras, ZI{\'{O}}{\L}KO AGH Akademia G{\'{o}}rniczo-Hutnicza im Stanis{\l}awa Staszica Krakowie, Elektroniki - 2009 - BAZA DANYCH NAGRA{\'{N}} MOWY EMOCJONA.pdf:pdf},
journal = {STUDIA INFORMATICA},
keywords = {detection of emotions in speech,speech recordings},
number = {182},
title = {{BAZA DANYCH NAGRA{\'{N}} MOWY EMOCJONALNEJ DATABASE OF EMOTIONAL SPEECH RECORDINGS}},
volume = {30},
year = {2009}
}
@inproceedings{Koolagudi2011,
abstract = {In this paper, prosodic analysis of speech segments is performed to recognise emotions. Speech signal is segmented into words and syllables. Energy and pitch parameters are extracted from utterances, words and syllables separately to develop emotion recognition models. Eight emotions (anger, disgust, fear, happy, neutral, sad, sarcastic and surprise) of simulated emotion speech corpus, IITKGP-SESC $\backslash$cite{\{}koolagudi2009{\}} are used in this work for recognition of emotions. Word boundaries are manually marked for 15 utterances of IITKGP-SESC. Syllable boundaries are detected using vowel onset points (VOPs) as anchor locations. Recognition performance of emotions using segmental level prosodic features is not found to be appreciable, but by combining spectral features along with prosodic features, emotion recognition performance is considerably improved. Support vector machines (SVM) and Gaussian mixture models (GMM) are used to develop emotion models to analyse different speech segments for emotion recognition.},
author = {Koolagudi, Shashidhar G. and Kumar, Nitin and Rao, K. Sreenivasa},
booktitle = {2011 International Conference on Devices and Communications, ICDeCom 2011 - Proceedings},
doi = {10.1109/ICDECOM.2011.5738536},
isbn = {9781424491902},
keywords = {Emotion recognition,Emotion verification,Energy,IITKGP-SESC,Pitch,SVM,Segmental level prosodic features,VOP},
title = {{Speech emotion recognition using segmental level prosodic analysis}},
year = {2011}
}
@article{Rao2013,
abstract = {In this paper, global and local prosodic features extracted from sentence, word and syllables are proposed for speech emotion or affect recognition. In this work, duration, pitch, and energy values are used to represent the prosodic information, for recognizing the emotions from speech. Global prosodic features represent the gross statistics such as mean, minimum, maximum, standard deviation, and slope of the prosodic contours. Local prosodic features represent the temporal dynamics in the prosody. In this work, global and local prosodic features are analyzed separately and in combination at different levels for the recognition of emotions. In this study, we have also explored the words and syllables at different positions (initial, middle, and ﬁnal) separately, to analyze their contribution towards the recognition of emotions. In this paper, all the studies are carried out using simulated Telugu emotion speech corpus (IITKGP-SESC). These results are compared with the results of internationally known Berlin emotion speech corpus (Emo-DB). Support vector machines are used to develop the emotion recognition models. The results indicate that, the recognition performance using local prosodic features is better compared to the performance of global prosodic features. Words in the ﬁnal position of the sentences, syllables in the ﬁnal position of the words exhibit more emotion discriminative information compared to the words and syllables present in the other positions.},
author = {Rao, K. Sreenivasa and Koolagudi, Shashidhar G. and Vempada, Ramu Reddy},
doi = {10.1007/s10772-012-9172-2},
isbn = {1381-2416},
issn = {13812416},
journal = {International Journal of Speech Technology},
number = {2},
title = {{Emotion recognition from speech using global and local prosodic features}},
volume = {16},
year = {2013}
}
@article{Wang2015,
abstract = {Recently, studies have been performed on harmony features for speech emotion recognition. It is found in our study that the first- and second-order differences of harmony features also play an important role in speech emotion recognition. Therefore, we propose a new Fourier parameter model using the perceptual content of voice quality and the first- and second-order differences for speaker-independent speech emotion recognition. Experimental results show that the proposed Fourier parameter (FP) features are effective in identifying various emotional states in speech signals. They improve the recognition rates over the methods using Mel frequency cepstral coefficient (MFCC) features by 16.2, 6.8 and 16.6 points on the German database (EMODB), Chinese language database (CASIA) and Chinese elderly emotion database (EESDB). In particular, when combining FP with MFCC, the recognition rates can be further improved on the aforementioned databases by 17.5, 10 and 10.5 points, respectively.},
author = {Wang, Kunxia and An, Ning and Li, Bing Nan and Zhang, Yanyong and Li, Lian},
doi = {10.1109/TAFFC.2015.2392101},
isbn = {1949-3045},
issn = {19493045},
journal = {IEEE Transactions on Affective Computing},
number = {1},
title = {{Speech emotion recognition using Fourier parameters}},
volume = {6},
year = {2015}
}
@article{Mencattini2014,
abstract = {Speech emotion recognition (SER) is a challenging framework in demanding human machine interaction systems. Standard approaches based on the categorical model of emotions reach low performance, probably due to the modelization of emotions as distinct and independent affective states. Starting from the recently investigated assumption on the dimensional circumplex model of emotions, SER systems are structured as the prediction of valence and arousal on a continuous scale in a two-dimensional domain. In this study, we propose the use of a PLS regression model, optimized according to specific features selection procedures and trained on the Italian speech corpus EMOVO, suggesting a way to automatically label the corpus in terms of arousal and valence. New speech features related to the speech amplitude modulation, caused by the slowly-varying articulatory motion, and standard features extracted from the pitch contour, have been included in the regression model. An average value for the coefficient of determination R2 of 0.72 (maximum value of 0.95 for fear and minimum of 0.60 for sadness) is obtained for the female model and a value for R2 of 0.81 (maximum value of 0.89 for anger and minimum value of 0.71 for joy) is obtained for the male model, over the seven primary emotions (including the neutral state). {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
author = {Mencattini, Arianna and Martinelli, Eugenio and Costantini, Giovanni and Todisco, Massimiliano and Basile, Barbara and Bozzali, Marco and {Di Natale}, Corrado},
doi = {10.1016/j.knosys.2014.03.019},
issn = {09507051},
journal = {Knowledge-Based Systems},
title = {{Speech emotion recognition using amplitude modulation parameters and a combined feature selection procedure}},
volume = {63},
year = {2014}
}
@article{Cao2015,
abstract = {We introduce a ranking approach for emotion recognition which naturally incorporates information about the general expressivity of speakers. We demonstrate that our approach leads to substantial gains in accuracy compared to conventional approaches. We train ranking SVMs for individual emotions, treating the data from each speaker as a separate query, and combine the predictions from all rankers to perform multi-class prediction. The ranking method provides two natural benefits. It captures speaker specific information even in speaker-independent training/testing conditions. It also incorporates the intuition that each utterance can express a mix of possible emotion and that considering the degree to which each emotion is expressed can be productively exploited to identify the dominant emotion. We compare the performance of the rankers and their combination to standard SVM classification approaches on two publicly available datasets of acted emotional speech, Berlin and LDC, as well as on spontaneous emotional data from the FAU Aibo dataset. On acted data, ranking approaches exhibit significantly better performance compared to SVM classification both in distinguishing a specific emotion from all others and in multi-class prediction. On the spontaneous data, which contains mostly neutral utterances with a relatively small portion of less intense emotional utterances, ranking-based classifiers again achieve much higher precision in identifying emotional utterances than conventional SVM classifiers. In addition, we discuss the complementarity of conventional SVM and ranking-based classifiers. On all three datasets we find dramatically higher accuracy for the test items on whose prediction the two methods agree compared to the accuracy of individual methods. Furthermore on the spontaneous data the ranking and standard classification are complementary and we obtain marked improvement when we combine the two classifiers by late-stage fusion.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Cao, Houwei and Verma, Ragini and Nenkova, Ani},
doi = {10.1016/j.csl.2014.01.003},
eprint = {NIHMS150003},
isbn = {2122633255},
issn = {10958363},
journal = {Computer Speech and Language},
number = {1},
pmid = {25422534},
title = {{Speaker-sensitive emotion recognition via ranking: Studies on acted and spontaneous speech}},
volume = {29},
year = {2015}
}
@article{Steidl2009,
author = {Steidl, Stefan},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Steidl - 2009 - Automatic Classification of Emotion-Related User States in Spontaneous Children's Speech.pdf:pdf},
title = {{Automatic Classification of Emotion-Related User States in Spontaneous Children's Speech}},
year = {2009}
}
@article{Elektrotechniki2014,
author = {Elektrotechniki, Wydzia{\l} and Automatyki, Informatyki},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elektrotechniki, Automatyki - 2014 - POLITECHNIKA {\L}{\'{O}}DZKA.pdf:pdf},
title = {{POLITECHNIKA {\L}{\'{O}}DZKA}},
year = {2014}
}
@article{,
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Emotion Detection from Speech.pdf:pdf},
title = {{Emotion Detection from Speech}}
}
@article{Verma,
abstract = {—Recent times have been marked with the increasing demand for more intelligent human computer interfaces. By adding emotion recognition abilities, voice based interfaces can be made more human centric. As natural languages do not share similar acoustic-phonetic features and vary in production of speech sound, the emotion recognition accuracy gets affected with respect to the user's language. This work aims at studying the patterns of stress and intonation for emotional speech in Hindi (Indo-Aryan language) and analyzing the influence of gender on speech emotion recognition accuracy. The paper proposes a combined system for gender distinction and emotion recognition by extracting basic prosodic and spectral speech features and also compares three different classification algorithms. The performed experimentation over a Hindi emotional corpus reveals that 78{\%} correct speech emotion recognition accuracy can obtained by adopting support vector machines for classification.},
author = {Verma, Devika and Mukhopadhyay, Debajyoti and Mark, Emmanuel},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Verma, Mukhopadhyay, Mark - Unknown - Role of Gender Influence in Vocal Hindi Conversations A Study on Speech Emotion Recognition.pdf:pdf},
keywords = {gender recognition,human computer intelligent interaction,prosodic features,spectral features,support vector machine,—speech emotion recognition},
title = {{Role of Gender Influence in Vocal Hindi Conversations: A Study on Speech Emotion Recognition}}
}
@article{Chatterjee2014,
abstract = {Despite their remarkable success in bringing spoken language to hearing impaired listeners, the signal transmitted through cochlear implants (CIs) remains impoverished in spectro-temporal fine structure. As a consequence, pitch-dominant information such as voice emotion, is diminished. For young children, the ability to correctly identify the mood/intent of the speaker (which may not always be visible in their facial expression) is an important aspect of social and linguistic development. Previous work in the field has shown that children with cochlear implants (cCI) have significant deficits in voice emotion recognition relative to their normally hearing peers (cNH). Here, we report on voice emotion recognition by a cohort of 36 school-aged cCI. Additionally, we provide for the first time, a comparison of their performance to that of cNH and NH adults (aNH) listening to CI simulations of the same stimuli. We also provide comparisons to the performance of adult listeners with CIs (aCI), most of whom learned language primarily through normal acoustic hearing. Results indicate that, despite strong variability, on average, cCI perform similarly to their adult counterparts; that both groups' mean performance is similar to aNHs' performance with 8-channel noise-vocoded speech; that cNH achieve excellent scores in voice emotion recognition with full-spectrum speech, but on average, show significantly poorer scores than aNH with 8-channel noise-vocoded speech. A strong developmental effect was observed in the cNH with noise-vocoded speech in this task. These results point to the considerable benefit obtained by cochlear-implanted children from their devices, but also underscore the need for further research and development in this important and neglected area.This article is part of a Special Issue entitled {\textless}Lasker Award{\textgreater}.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Chatterjee, Monita and Zion, Danielle J. and Deroche, Mickael L. and Burianek, Brooke A. and Limb, Charles J. and Goren, Alison P. and Kulkarni, Aditya M. and Christensen, Julie A.},
doi = {10.1016/j.heares.2014.10.003},
eprint = {15334406},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatterjee et al. - 2014 - Voice emotion recognition by cochlear-implanted children and their normally-hearing peers.pdf:pdf},
isbn = {0378-5955},
issn = {18785891},
journal = {Hearing Research},
pmid = {25448167},
title = {{Voice emotion recognition by cochlear-implanted children and their normally-hearing peers}},
year = {2014}
}
@article{Luengo2010,
abstract = {The definition of parameters is a crucial step in the development of a system for identifying emotions in speech. Although there is no agreement on which are the best features for this task, it is generally accepted that prosody carries most of the emotional information. Most works in the field use some kind of prosodic features, often in combination with spectral and voice quality parametrizations. Nevertheless, no systematic study has been done comparing these features. This paper presents the analysis of the characteristics of features derived from prosody, spectral envelope, and voice quality as well as their capability to discriminate emotions. In addition, early fusion and late fusion techniques for combining different information sources are evaluated. The results of this analysis are validated with experimental automatic emotion identification tests. Results suggest that spectral envelope features outperform the prosodic ones. Even when different parametrizations are combined, the late fusion of long-term spectral statistics with short-term spectral envelope parameters provides an accuracy comparable to that obtained when all parametrizations are combined.},
author = {Luengo, Iker and Navas, Eva and Hernaez, Inmaculada},
doi = {10.1109/TMM.2010.2051872},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luengo, Navas, Hernaez - 2010 - Feature analysis and evaluation for automatic emotion identification in speech.pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Emotion identification,information fusion,parametrization},
title = {{Feature analysis and evaluation for automatic emotion identification in speech}},
year = {2010}
}
@article{Bonora2011,
abstract = {Patients with chronic medial temporal lobe epilepsy (MTLE) can be impaired in different tasks that evaluate emotional or social abilities. In particular, the recognition of facial emotions can be affected (Meletti S, Benuzzi F, Rubboli G, et al. Neurology 2003;60:426-31. Meletti S, Benuzzi F, Cantalupo G, Rubboli G, Tassinari CA, Nichelli P. Epilepsia 2009;50:1547-59). To better understand the nature of emotion recognition deficits in MTLE we investigated the decoding of basic emotions in the visual (facial expression) and auditory (emotional prosody) domains in 41 patients. Results showed deficits in the recognition of both facial and vocal expression of emotions, with a strong correlation between performances across the two tasks. No correlation between emotion recognition and measures of IQ, quality of life (QOLIE-31), and depression (Beck Depression Inventory) was significant, except for a weak correlation between prosody recognition and IQ. These data suggest that emotion recognition impairment in MTLE is not dependent on the sensory channel through which the emotional stimulus is transmitted. Moreover, these findings support the notion that emotional processing is at least partly independent of measures of cognitive intelligence. {\textcopyright} 2011 Elsevier Inc.},
author = {Bonora, Annalisa and Benuzzi, Francesca and Monti, Giulia and Mirandola, Laura and Pugnaghi, Matteo and Nichelli, Paolo and Meletti, Stefano},
doi = {10.1016/j.yebeh.2011.01.027},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonora et al. - 2011 - Recognition of emotions from faces and voices in medial temporal lobe epilepsy.pdf:pdf},
isbn = {1525-5069 (Electronic)$\backslash$n1525-5050 (Linking)},
issn = {15255050},
journal = {Epilepsy and Behavior},
keywords = {Amygdala,Emotion,Emotional prosody,Facial emotion recognition,Facial expressions,Medial temporal lobe epilepsy},
pmid = {21459049},
title = {{Recognition of emotions from faces and voices in medial temporal lobe epilepsy}},
year = {2011}
}
@misc{Strelnikov2015,
abstract = {In this article, we review the PET neuroimaging literature, which indicates peculiarities of brain networks involved in speech restoration after cochlear implantation. We consider data on implanted patients during stimulation as well as during resting state, which indicates basic long-term reorganisation of brain functional architecture. On the basis of our analysis of neuroimaging literature and considering our own studies, we indicate that auditory recovery in deaf patients after cochlear implantation partly relies on visual cues. The brain develops mechanisms of audio-visual integration as a strategy to achieve high levels of speech recognition. It turns out that this neuroimaging evidence is in line with behavioural findings of better audiovisual integration in these patients. Thus, strong visually and audio-visually based rehabilitation during the first months after cochlear implantation would significantly improve and fasten the functional recovery of speech intelligibility and other auditory functions in these patients. We provide perspectives for further neuroimaging studies in cochlear implanted patients, which would help understand brain organisation to restore auditory cognitive processing in the implanted patients and would potentially suggest novel approaches for their rehabilitation.This article is part of a Special Issue entitled {\textless}Lasker Award{\textgreater}.},
author = {Strelnikov, K. and Marx, M. and Lagleyre, S. and Fraysse, B. and Deguine, O. and Barone, P.},
booktitle = {Hearing Research},
doi = {10.1016/j.heares.2014.10.001},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Strelnikov et al. - 2015 - PET-imaging of brain plasticity after cochlear implantation.pdf:pdf},
isbn = {0378-5955},
issn = {18785891},
pmid = {25448166},
title = {{PET-imaging of brain plasticity after cochlear implantation}},
year = {2015}
}
@misc{Laback2015,
abstract = {Bilateral cochlear implantation is increasingly becoming the standard in the clinical treatment of bilateral deafness. The main motivation is to provide users of bilateral cochlear implants (CIs) access to binaural cues essential for localizing sound sources and understanding speech in environments of interfering sounds. One of those cues, interaural level differences, can be perceived well by CI users to allow some basic left versus right localization. However, interaural time differences (ITDs) which are important for localization of low-frequency sounds and spatial release from masking are not adequately represented by clinical envelope-based CI systems. Here, we first review the basic ITD sensitivity of CI users, particularly their dependence on stimulation parameters like stimulation rate and place, modulation rate, and envelope shape in single-electrode stimulation, as well as stimulation level, electrode spacing, and monaural across-electrode timing in multiple-electrode stimulation. Then, we discuss factors involved in ITD perception in electric hearing including the match between highly phase-locked electric auditory nerve response properties and binaural cell properties, the restricted stimulation of apical tonotopic pathways, channel interactions in multiple-electrode stimulation, and the onset age of binaural auditory input. Finally, we present clinically available CI stimulation strategies and experimental strategies aiming at improving listeners' access to ITD cues.This article is part of a Special Issue entitled {\textless}Lasker Award{\textgreater}.},
author = {Laback, Bernhard and Egger, Katharina and Majdak, Piotr},
booktitle = {Hearing Research},
doi = {10.1016/j.heares.2014.10.004},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laback, Egger, Majdak - 2015 - Perception and coding of interaural time differences with bilateral cochlear implants.pdf:pdf},
isbn = {0378-5955},
issn = {18785891},
pmid = {25456088},
title = {{Perception and coding of interaural time differences with bilateral cochlear implants}},
year = {2015}
}
@article{Buss,
abstract = {Monaural envelope correlation perception concerns the ability of listeners to discriminate stimuli based on the degree of correlation between the temporal envelopes of two or more frequency-separated bands of noise [Richards, J. Acoust. Soc. Am. 82, 1621–1630 (1987)]. Previous work has examined this ability for relatively narrow bandwidths, generally 100 Hz or less. The present experi-ment explored a wide range of bandwidths, from 25 to 1600 Hz, which included bands narrower and wider than a critical bandwidth. Stimuli were pairs of noise bands separated by a 500-Hz-wide spec-tral gap centered on 2250 Hz. The magnitude spectra of the pair of comodulated bands were either identical or reflected around the midpoint of the band, and performance was assessed with and with-out a low-pass noise masker. Although discrimination was best for intermediate bandwidths, mean performance was above chance for all bandwidths tested. Data were similar for stimuli with identi-cal and reflected magnitude spectra, and for stimuli with and without the low-pass masker. The one exception was particularly good performance for intermediate-bandwidth stimuli with identical spectra, for which some listeners reported hearing a tonal cue. Results indicate that listeners are flexi-ble in selecting spectral regions upon which to base across-frequency comparisons.},
author = {Buss, Emily and {Hall Iii}, Joseph W and Grose, John H},
doi = {10.1121/1.4768887},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buss, Hall Iii, Grose - Unknown - Monaural envelope correlation perception for bands narrower or wider than a critical band(2).pdf:pdf},
pages = {405--416},
title = {{Monaural envelope correlation perception for bands narrower or wider than a critical band}}
}
@inproceedings{Wang2014,
abstract = {Research on the vocal expression of emotion has long since used a " fishing expedition" approach to find acoustic markers for emotion categories and dimensions. Although partially successful, the underlying mechanisms have not yet been elucidated. To illustrate that this research can profit from considering the underlying voice production mechanism, we specifically analyzed short affect bursts (sustained/a/vowels produced by 10 professional actors for five emotions) according to physiological variations in phonation (using acoustic parameters derived from the acoustic signal and the inverse filter estimated voice source waveform). Results show significant emotion main effects for 11 of 12 parameters. Subsequent principal components analysis revealed three components that explain acoustic variations due to emotion, including " tension," " perturbation," and " voicing frequency." These results suggest that future work may benefit from theory-guided development of parameters to assess differences in physiological voice production mechanisms in the vocal expression of different emotions. ?? 2011 Elsevier B.V.},
author = {Wang, Ting and Ding, Hongwei and Kuang, Jianjing and Ma, Qiuwu},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
doi = {10.1016/j.biopsycho.2011.02.010},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2014 - Mapping emotions into acoustic space The role of voice quality.pdf:pdf},
isbn = {1873-6246 (Electronic)$\backslash$n0301-0511 (Linking)},
issn = {19909772},
keywords = {Emotion dimensions,Perception,Production,Vocal emotions,Voice quality},
pmid = {21354259},
title = {{Mapping emotions into acoustic space: The role of voice quality}},
year = {2014}
}
@article{Svirsky2015,
abstract = {What is the best way to help humans adapt to a distorted sensory input? Interest in this question is more than academic. The answer may help facilitate auditory learning by people who became deaf after learning language and later received a cochlear implant (a neural prosthesis that restores hearing through direct electrical stimulation of the auditory nerve). There is evidence that some cochlear implants (which provide information that is spectrally degraded to begin with) stimulate neurons with higher characteristic frequency than the acoustic frequency of the original stimulus. In other words, the stimulus is shifted in frequency with respect to what the listener expects to hear. This frequency misalignment may have a negative influence on speech perception by CI users. However, a perfect frequency-place alignment may result in the loss of important low frequency speech information. A trade-off may involve a gradual approach: start with correct frequency-place alignment to allow listeners to adapt to the spectrally degraded signal first, and then gradually increase the frequency shift to allow them to adapt to it over time. We used an acoustic model of a cochlear implant to measure adaptation to a frequency-shifted signal, using either the gradual approach or the "standard" approach (sudden imposition of the frequency shift). Listeners in both groups showed substantial auditory learning, as measured by increases in speech perception scores over the course of fifteen one-hour training sessions. However, the learning process was faster for listeners who were exposed to the gradual approach. These results suggest that gradual rather than sudden exposure may facilitate perceptual learning in the face of a spectrally degraded, frequency-shifted input.This article is part of a Special Issue entitled {\textless}Lasker Award{\textgreater}.},
author = {Svirsky, Mario A. and Talavage, Thomas M. and Sinha, Shivank and Neuburger, Heidi and Azadpour, Mahan},
doi = {10.1016/j.heares.2014.10.008},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Svirsky et al. - 2015 - Gradual adaptation to auditory frequency mismatch.pdf:pdf},
isbn = {0378-5955},
issn = {18785891},
journal = {Hearing Research},
pmid = {25445816},
title = {{Gradual adaptation to auditory frequency mismatch}},
year = {2015}
}
@article{Palmer2005,
abstract = {There has been some debate recently over the scoring, reliability and factor structure of ability measures of emotional intelligence (EI). This study examined these three psychometric properties with the most recent ability test of EI, the Mayer - Salovey - Caruso Emotional Intelligence Test (MSCEIT V2.0; Mayer, Salovey, {\&} Caruso, [Mayer, J. D., Salovey, {\&} P., Caruso, (2000). Models of emotional intelligence. In R. J., Sternberg (Ed.). Handbook of intelligence (pp. 396-420). New York: Cambridge; Mayer, J. D., Salovey, P., {\&} Caruso, D. R., (2000). The Mayer, Salovey, and Caruso emotional intelligence test: Technical manual. Toronto, ON: MHS]), with a sample (n=431) drawn from the general population. The reliability of the MSCEIT at the total scale, area and branch levels was found to be good, although the reliability of most of the subscales was relatively low. Consistent with previous findings, there was a high level of convergence between the alternative scoring methods (consensus and expert). However, unlike Mayer et al.'s [Mayer, J. D., Salovey, P., Caruso, D. R., {\&} Sitarenios, G. (2003). Measuring emotional intelligence with the MSCEIT V2. 0. Emotion, 3, 97-105.] contentions, there was only partial support for their four-factor model of EI. A model with a general first-order factor of EI and a three first-order branch level factors was determined to be the best fitting model. There was no support for the Experiential Area level factor, nor was there support for the Facilitating Branch level factor. These results were replicated closely using the Mayer et al. [Mayer, J. D., Salovey, P., Caruso, D. R., {\&} Sitarenios, G., (2003). Measuring emotional intelligence with the MSCEIT V2. 0. Emotion, 3, 97-105.] data. The results are discussed in light of the close comparability of the two scoring methods. Furthermore, the fundamental limitations of the MSCEIT V2.0, with respect to the inadequate number of subscales theorized to measure each branch level factor are identified and discussed. ?? 2004 Elsevier Inc. All rights reserved.},
author = {Palmer, Benjamin R. and Gignac, Gilles and Manocha, Ramesh and Stough, Con},
doi = {10.1016/j.intell.2004.11.003},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Palmer et al. - 2005 - A psychometric evaluation of the Mayer-Salovey-Caruso Emotional Intelligence Test Version 2.0.pdf:pdf},
isbn = {Print 0160-2896 Elsevier Science Electronic},
issn = {01602896},
journal = {Intelligence},
keywords = {Emotional competencies,Emotional intelligence,Emotions,Factor structure,Reliability},
title = {{A psychometric evaluation of the Mayer-Salovey-Caruso Emotional Intelligence Test Version 2.0}},
year = {2005}
}
@article{Thingujam2012,
abstract = {One key criterion for whether Emotional Intelligence (EI) truly fits the definition of " intelligence" is that individual branches of EI should converge. However, for performance tests that measure actual ability, such convergence has been elusive. Consistent with theoretical perspectives for intelligence, we approach this question using EI measures that have objective standards for right answers. Examining emotion recognition through the voice-that is, the ability to judge an actor's intended portrayal-and emotional understanding-that is, the ability to understand relationships and transitions among emotions-we find substantial convergence, r=.53. Results provide new data to inform the often heated debate about the validity of EI, and further the basis of optimism that EI may truly be considered intelligence. {\textcopyright} 2012 Elsevier Inc.},
author = {Thingujam, Nutankumar S. and Laukka, Petri and Elfenbein, Hillary Anger},
doi = {10.1016/j.jrp.2012.02.005},
isbn = {0092-6566},
issn = {00926566},
journal = {Journal of Research in Personality},
keywords = {Ability measure,Emotion recognition,Emotional intelligence,Emotional understanding},
title = {{Distinct emotional abilities converge: Evidence from emotional understanding and emotion recognition through the voice}},
year = {2012}
}
@article{Noroozi2017,
author = {Noroozi, Fatemeh and Sapi{\'{n}}ski, Tomasz and Kami{\'{n}}ska, Dorota and Anbarjafari, Gholamreza},
doi = {10.1007/s10772-017-9396-2},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Noroozi et al. - 2017 - Vocal-based emotion recognition using random forests and decision tree.pdf:pdf},
isbn = {1077201793962},
issn = {15728110},
journal = {International Journal of Speech Technology},
keywords = {Decision tree classifier,Human–computer interaction,Random forests,Vocal emotion recognition},
title = {{Vocal-based emotion recognition using random forests and decision tree}},
year = {2017}
}
@article{Vogt,
abstract = {We present a data-mining experiment on feature selection for automatic emotion recognition. Starting from more than 1000 features derived from pitch, energy and MFCC time series, the most relevant features in respect to the data are selected from this set by removing correlated features. The features selected for acted and realistic emotions are anal-ysed and show significant differences. All features are com-puted automatically and we also contrast automatically with manually units of analysis. A higher degree of automation did not prove to be a disadvantage in terms of recognition accuracy.},
author = {Vogt, Thurid and Andr{\'{e}}, Elisabeth},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vogt, Andr{\'{e}} - Unknown - COMPARING FEATURE SETS FOR ACTED AND SPONTANEOUS SPEECH IN VIEW OF AUTOMATIC EMOTION RECOGNITION.pdf:pdf},
title = {{COMPARING FEATURE SETS FOR ACTED AND SPONTANEOUS SPEECH IN VIEW OF AUTOMATIC EMOTION RECOGNITION}}
}
@article{Nwe2003,
abstract = {In emotion classification of speech signals, the popular features employed are statistics of fundamental frequency, energy contour, duration of silence and voice quality. However, the performance of systems employing these features degrades substantially when more than two categories of emotion are to be classified. In this paper, a text independent method of emotion classification of speech is proposed. The proposed method makes use of short time log frequency power coefficients (LFPC) to represent the speech signals and a discrete hidden Markov model (HMM) as the classifier. The emotions are classified into six categories. The category labels used are, the archetypal emotions of Anger, Disgust, Fear, Joy, Sadness and Surprise. A database consisting of 60 emotional utterances, each from twelve speakers is constructed and used to train and test the proposed system. Performance of the LFPC feature parameters is compared with that of the linear prediction Cepstral coefficients (LPCC) and mel-frequency Cepstral coefficients (MFCC) feature parameters commonly used in speech recognition systems. Results show that the proposed system yields an average accuracy of 78{\%} and the best accuracy of 96{\%} in the classification of six emotions. This is beyond the 17{\%} chances by a random hit for a sample set of 6 categories. Results also reveal that LFPC is a better choice as feature parameters for emotion classification than the traditional feature parameters. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Nwe, Tin Lay and Foo, Say Wei and {De Silva}, Liyanage C.},
doi = {10.1016/S0167-6393(03)00099-2},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nwe, Foo, De Silva - 2003 - Speech emotion recognition using hidden Markov models.pdf:pdf},
isbn = {0167-6393},
issn = {01676393},
journal = {Speech Communication},
keywords = {Emotional speech,Hidden Markov model,Human communication,Log frequency power coefficients,Recognition of emotion},
title = {{Speech emotion recognition using hidden Markov models}},
year = {2003}
}
@article{Morrison2007,
abstract = {Machine-based emotional intelligence is a requirement for more natural interaction between humans and computer interfaces and a basic level of accurate emotion perception is needed for computer systems to respond adequately to human emotion. Humans convey emotional information both intentionally and unintentionally via speech patterns. These vocal patterns are perceived and understood by listeners during conversation. This research aims to improve the automatic perception of vocal emotion in two ways. First, we compare two emotional speech data sources: natural, spontaneous emotional speech and acted or portrayed emotional speech. This comparison demonstrates the advantages and disadvantages of both acquisition methods and how these methods affect the end application of vocal emotion recognition. Second, we look at two classification methods which have not been applied in this field: stacked generalisation and unweighted vote. We show how these techniques can yield an improvement over traditional classification methods. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Morrison, Donn and Wang, Ruili and {De Silva}, Liyanage C.},
doi = {10.1016/j.specom.2006.11.004},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morrison, Wang, De Silva - 2007 - Ensemble methods for spoken emotion recognition in call-centres.pdf:pdf},
issn = {01676393},
journal = {Speech Communication},
keywords = {Affect recognition,Emotion recognition,Ensemble methods,Speech databases,Speech processing},
title = {{Ensemble methods for spoken emotion recognition in call-centres}},
year = {2007}
}
@article{Anagnostopoulos2012,
abstract = {Speaker emotion recognition is achieved through processing methods that include isolation of the speech signal and extraction of selected features for the final classi- fication. In terms of acoustics, speech processing techniques offer extremely valuable para- linguistic information derived mainly from prosodic and spectral features. In some cases, the process is assisted by speech recognition systems,which contribute to the classification using linguistic information. Both frameworks deal with a very challenging problem, as emotional states do not have clear-cut boundaries and often differ from person to person. In this article, research papers that investigate emotion recognition from audio channels are surveyed and classified, basedmostly on extracted and selected features and their classificationmethodol- ogy. Important topics from different classification techniques, such as databases available for experimentation, appropriate feature extraction and selection methods, classifiers and per- formance issues are discussed, with emphasis on research published in the last decade. This survey also provides a discussion on open trends, along with directions for future research on this topic.},
author = {Anagnostopoulos, Christos Nikolaos and Iliou, Theodoros and Giannoukos, Ioannis},
doi = {10.1007/s10462-012-9368-5},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anagnostopoulos, Iliou, Giannoukos - 2012 - Features and classifiers for emotion recognition from speech a survey from 2000 to 2011.pdf:pdf},
issn = {15737462},
journal = {Artificial Intelligence Review},
keywords = {Classifiers,Emotion recognition,Speech features},
title = {{Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011}},
year = {2012}
}
@article{Kostoulas,
abstract = {In the present work we aim at performance optimization of a speaker-independent emotion recognition system through speech feature selection pro-cess. Specifically, relying on the speech feature set defined in the Interspeech 2009 Emotion Challenge, we studied the relative importance of the individual speech parameters, and based on their ranking, a subset of speech parameters that offered advantageous performance was selected. The affect-emotion recog-nizer utilized here relies on a GMM-UBM-based classifier. In all experiments, we followed the experimental setup defined by the Interspeech 2009 Emotion Challenge, utilizing the FAU Aibo Emotion Corpus of spontaneous, emotionally coloured speech. The experimental results indicate that the correct choice of the speech parameters can lead to better performance than the baseline one.},
author = {Kostoulas, Theodoros and Ganchev, Todor and Lazaridis, Alexandros and Fakotakis, Nikos},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kostoulas et al. - Unknown - Enhancing Emotion Recognition from Speech through Feature Selection.pdf:pdf},
keywords = {affect recognition,emotion recognition,feature selection,real-world data},
title = {{Enhancing Emotion Recognition from Speech through Feature Selection}}
}
