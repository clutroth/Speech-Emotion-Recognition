@article{Igras2009,
abstract = {Streszczenie. Artyku{\l} prezentuje opracowan{\c{a}} w AGH baz{\c{e}} danych nagra{\'{n}} mowy emocjonalnej zgromadzonej w celu bada{\'{n}} nad zawarto{\'{s}}ci{\c{a}} afektywn{\c{a}} sygna{\l}u mowy. Opisano spos{\'{o}}b rejestracji, parametry, struktur{\c{e}}, metadane i licencj{\c{e}} bazy danych. Przedstawiono przyk{\l}adowe zastosowania do opracowania metod detekcji stan{\'{o}}w emocjonalnych w g{\l}osie oraz normalizacji nagra{\'{n}} na potrzeby ASR. S{\l}owa kluczowe: nagrania mowy, detekcja emocji w g{\l}osie Summary. The paper presents a database of emotional speech recordings collected in AGH for research on affective content of speech signal. We describe the method of data acquisition, the parameters, structure, metadata and license. We present example applications for development of the methods of emotions detection in voice and emotional speech normalization for ASR.},
author = {Igras, Magdalena and {ZI{\'{O}}{\L}KO AGH Akademia G{\'{o}}rniczo-Hutnicza im Stanis{\l}awa Staszica Krakowie}, Bartosz and Elektroniki, Katedra},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Igras, ZI{\'{O}}{\L}KO AGH Akademia G{\'{o}}rniczo-Hutnicza im Stanis{\l}awa Staszica Krakowie, Elektroniki - 2009 - BAZA DANYCH NAGRA{\'{N}} MOWY EMOCJONA.pdf:pdf},
journal = {STUDIA INFORMATICA},
keywords = {detection of emotions in speech,speech recordings},
number = {182},
title = {{BAZA DANYCH NAGRA{\'{N}} MOWY EMOCJONALNEJ DATABASE OF EMOTIONAL SPEECH RECORDINGS}},
volume = {30},
year = {2009}
}
@article{Noroozi2017,
author = {Noroozi, Fatemeh and Sapi{\'{n}}ski, Tomasz and Kami{\'{n}}ska, Dorota and Anbarjafari, Gholamreza},
doi = {10.1007/s10772-017-9396-2},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Noroozi et al. - 2017 - Vocal-based emotion recognition using random forests and decision tree.pdf:pdf},
isbn = {1077201793962},
issn = {15728110},
journal = {International Journal of Speech Technology},
keywords = {Decision tree classifier,Human–computer interaction,Random forests,Vocal emotion recognition},
title = {{Vocal-based emotion recognition using random forests and decision tree}},
year = {2017}
}
@article{Kostoulas,
abstract = {In the present work we aim at performance optimization of a speaker-independent emotion recognition system through speech feature selection pro-cess. Specifically, relying on the speech feature set defined in the Interspeech 2009 Emotion Challenge, we studied the relative importance of the individual speech parameters, and based on their ranking, a subset of speech parameters that offered advantageous performance was selected. The affect-emotion recog-nizer utilized here relies on a GMM-UBM-based classifier. In all experiments, we followed the experimental setup defined by the Interspeech 2009 Emotion Challenge, utilizing the FAU Aibo Emotion Corpus of spontaneous, emotionally coloured speech. The experimental results indicate that the correct choice of the speech parameters can lead to better performance than the baseline one.},
author = {Kostoulas, Theodoros and Ganchev, Todor and Lazaridis, Alexandros and Fakotakis, Nikos},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kostoulas et al. - Unknown - Enhancing Emotion Recognition from Speech through Feature Selection.pdf:pdf},
keywords = {affect recognition,emotion recognition,feature selection,real-world data},
title = {{Enhancing Emotion Recognition from Speech through Feature Selection}}
}
@inproceedings{Wang2014,
abstract = {Research on the vocal expression of emotion has long since used a " fishing expedition" approach to find acoustic markers for emotion categories and dimensions. Although partially successful, the underlying mechanisms have not yet been elucidated. To illustrate that this research can profit from considering the underlying voice production mechanism, we specifically analyzed short affect bursts (sustained/a/vowels produced by 10 professional actors for five emotions) according to physiological variations in phonation (using acoustic parameters derived from the acoustic signal and the inverse filter estimated voice source waveform). Results show significant emotion main effects for 11 of 12 parameters. Subsequent principal components analysis revealed three components that explain acoustic variations due to emotion, including " tension," " perturbation," and " voicing frequency." These results suggest that future work may benefit from theory-guided development of parameters to assess differences in physiological voice production mechanisms in the vocal expression of different emotions. ?? 2011 Elsevier B.V.},
author = {Wang, Ting and Ding, Hongwei and Kuang, Jianjing and Ma, Qiuwu},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
doi = {10.1016/j.biopsycho.2011.02.010},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2014 - Mapping emotions into acoustic space The role of voice quality.pdf:pdf},
isbn = {1873-6246 (Electronic)$\backslash$n0301-0511 (Linking)},
issn = {19909772},
keywords = {Emotion dimensions,Perception,Production,Vocal emotions,Voice quality},
pmid = {21354259},
title = {{Mapping emotions into acoustic space: The role of voice quality}},
year = {2014}
}
@article{Thingujam2012,
abstract = {One key criterion for whether Emotional Intelligence (EI) truly fits the definition of " intelligence" is that individual branches of EI should converge. However, for performance tests that measure actual ability, such convergence has been elusive. Consistent with theoretical perspectives for intelligence, we approach this question using EI measures that have objective standards for right answers. Examining emotion recognition through the voice-that is, the ability to judge an actor's intended portrayal-and emotional understanding-that is, the ability to understand relationships and transitions among emotions-we find substantial convergence, r=.53. Results provide new data to inform the often heated debate about the validity of EI, and further the basis of optimism that EI may truly be considered intelligence. {\textcopyright} 2012 Elsevier Inc.},
author = {Thingujam, Nutankumar S. and Laukka, Petri and Elfenbein, Hillary Anger},
doi = {10.1016/j.jrp.2012.02.005},
isbn = {0092-6566},
issn = {00926566},
journal = {Journal of Research in Personality},
keywords = {Ability measure,Emotion recognition,Emotional intelligence,Emotional understanding},
title = {{Distinct emotional abilities converge: Evidence from emotional understanding and emotion recognition through the voice}},
year = {2012}
}
@article{Luengo2010,
abstract = {The definition of parameters is a crucial step in the development of a system for identifying emotions in speech. Although there is no agreement on which are the best features for this task, it is generally accepted that prosody carries most of the emotional information. Most works in the field use some kind of prosodic features, often in combination with spectral and voice quality parametrizations. Nevertheless, no systematic study has been done comparing these features. This paper presents the analysis of the characteristics of features derived from prosody, spectral envelope, and voice quality as well as their capability to discriminate emotions. In addition, early fusion and late fusion techniques for combining different information sources are evaluated. The results of this analysis are validated with experimental automatic emotion identification tests. Results suggest that spectral envelope features outperform the prosodic ones. Even when different parametrizations are combined, the late fusion of long-term spectral statistics with short-term spectral envelope parameters provides an accuracy comparable to that obtained when all parametrizations are combined.},
author = {Luengo, Iker and Navas, Eva and Hernaez, Inmaculada},
doi = {10.1109/TMM.2010.2051872},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luengo, Navas, Hernaez - 2010 - Feature analysis and evaluation for automatic emotion identification in speech.pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Emotion identification,information fusion,parametrization},
title = {{Feature analysis and evaluation for automatic emotion identification in speech}},
year = {2010}
}
@article{Mencattini2014,
abstract = {Speech emotion recognition (SER) is a challenging framework in demanding human machine interaction systems. Standard approaches based on the categorical model of emotions reach low performance, probably due to the modelization of emotions as distinct and independent affective states. Starting from the recently investigated assumption on the dimensional circumplex model of emotions, SER systems are structured as the prediction of valence and arousal on a continuous scale in a two-dimensional domain. In this study, we propose the use of a PLS regression model, optimized according to specific features selection procedures and trained on the Italian speech corpus EMOVO, suggesting a way to automatically label the corpus in terms of arousal and valence. New speech features related to the speech amplitude modulation, caused by the slowly-varying articulatory motion, and standard features extracted from the pitch contour, have been included in the regression model. An average value for the coefficient of determination R2 of 0.72 (maximum value of 0.95 for fear and minimum of 0.60 for sadness) is obtained for the female model and a value for R2 of 0.81 (maximum value of 0.89 for anger and minimum value of 0.71 for joy) is obtained for the male model, over the seven primary emotions (including the neutral state). {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
author = {Mencattini, Arianna and Martinelli, Eugenio and Costantini, Giovanni and Todisco, Massimiliano and Basile, Barbara and Bozzali, Marco and {Di Natale}, Corrado},
doi = {10.1016/j.knosys.2014.03.019},
issn = {09507051},
journal = {Knowledge-Based Systems},
title = {{Speech emotion recognition using amplitude modulation parameters and a combined feature selection procedure}},
volume = {63},
year = {2014}
}
@article{Elektrotechniki2014,
author = {Elektrotechniki, Wydzia{\l} and Automatyki, Informatyki},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elektrotechniki, Automatyki - 2014 - POLITECHNIKA {\L}{\'{O}}DZKA.pdf:pdf},
title = {{POLITECHNIKA {\L}{\'{O}}DZKA}},
year = {2014}
}
@article{Morrison2007,
abstract = {Machine-based emotional intelligence is a requirement for more natural interaction between humans and computer interfaces and a basic level of accurate emotion perception is needed for computer systems to respond adequately to human emotion. Humans convey emotional information both intentionally and unintentionally via speech patterns. These vocal patterns are perceived and understood by listeners during conversation. This research aims to improve the automatic perception of vocal emotion in two ways. First, we compare two emotional speech data sources: natural, spontaneous emotional speech and acted or portrayed emotional speech. This comparison demonstrates the advantages and disadvantages of both acquisition methods and how these methods affect the end application of vocal emotion recognition. Second, we look at two classification methods which have not been applied in this field: stacked generalisation and unweighted vote. We show how these techniques can yield an improvement over traditional classification methods. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Morrison, Donn and Wang, Ruili and {De Silva}, Liyanage C.},
doi = {10.1016/j.specom.2006.11.004},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morrison, Wang, De Silva - 2007 - Ensemble methods for spoken emotion recognition in call-centres.pdf:pdf},
issn = {01676393},
journal = {Speech Communication},
keywords = {Affect recognition,Emotion recognition,Ensemble methods,Speech databases,Speech processing},
title = {{Ensemble methods for spoken emotion recognition in call-centres}},
year = {2007}
}
@article{Cao2015,
abstract = {We introduce a ranking approach for emotion recognition which naturally incorporates information about the general expressivity of speakers. We demonstrate that our approach leads to substantial gains in accuracy compared to conventional approaches. We train ranking SVMs for individual emotions, treating the data from each speaker as a separate query, and combine the predictions from all rankers to perform multi-class prediction. The ranking method provides two natural benefits. It captures speaker specific information even in speaker-independent training/testing conditions. It also incorporates the intuition that each utterance can express a mix of possible emotion and that considering the degree to which each emotion is expressed can be productively exploited to identify the dominant emotion. We compare the performance of the rankers and their combination to standard SVM classification approaches on two publicly available datasets of acted emotional speech, Berlin and LDC, as well as on spontaneous emotional data from the FAU Aibo dataset. On acted data, ranking approaches exhibit significantly better performance compared to SVM classification both in distinguishing a specific emotion from all others and in multi-class prediction. On the spontaneous data, which contains mostly neutral utterances with a relatively small portion of less intense emotional utterances, ranking-based classifiers again achieve much higher precision in identifying emotional utterances than conventional SVM classifiers. In addition, we discuss the complementarity of conventional SVM and ranking-based classifiers. On all three datasets we find dramatically higher accuracy for the test items on whose prediction the two methods agree compared to the accuracy of individual methods. Furthermore on the spontaneous data the ranking and standard classification are complementary and we obtain marked improvement when we combine the two classifiers by late-stage fusion.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Cao, Houwei and Verma, Ragini and Nenkova, Ani},
doi = {10.1016/j.csl.2014.01.003},
eprint = {NIHMS150003},
isbn = {2122633255},
issn = {10958363},
journal = {Computer Speech and Language},
number = {1},
pmid = {25422534},
title = {{Speaker-sensitive emotion recognition via ranking: Studies on acted and spontaneous speech}},
volume = {29},
year = {2015}
}
@inproceedings{Koolagudi2011,
abstract = {In this paper, prosodic analysis of speech segments is performed to recognise emotions. Speech signal is segmented into words and syllables. Energy and pitch parameters are extracted from utterances, words and syllables separately to develop emotion recognition models. Eight emotions (anger, disgust, fear, happy, neutral, sad, sarcastic and surprise) of simulated emotion speech corpus, IITKGP-SESC $\backslash$cite{\{}koolagudi2009{\}} are used in this work for recognition of emotions. Word boundaries are manually marked for 15 utterances of IITKGP-SESC. Syllable boundaries are detected using vowel onset points (VOPs) as anchor locations. Recognition performance of emotions using segmental level prosodic features is not found to be appreciable, but by combining spectral features along with prosodic features, emotion recognition performance is considerably improved. Support vector machines (SVM) and Gaussian mixture models (GMM) are used to develop emotion models to analyse different speech segments for emotion recognition.},
author = {Koolagudi, Shashidhar G. and Kumar, Nitin and Rao, K. Sreenivasa},
booktitle = {2011 International Conference on Devices and Communications, ICDeCom 2011 - Proceedings},
doi = {10.1109/ICDECOM.2011.5738536},
isbn = {9781424491902},
keywords = {Emotion recognition,Emotion verification,Energy,IITKGP-SESC,Pitch,SVM,Segmental level prosodic features,VOP},
title = {{Speech emotion recognition using segmental level prosodic analysis}},
year = {2011}
}
@article{Vogt,
abstract = {We present a data-mining experiment on feature selection for automatic emotion recognition. Starting from more than 1000 features derived from pitch, energy and MFCC time series, the most relevant features in respect to the data are selected from this set by removing correlated features. The features selected for acted and realistic emotions are anal-ysed and show significant differences. All features are com-puted automatically and we also contrast automatically with manually units of analysis. A higher degree of automation did not prove to be a disadvantage in terms of recognition accuracy.},
author = {Vogt, Thurid and Andr{\'{e}}, Elisabeth},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vogt, Andr{\'{e}} - Unknown - COMPARING FEATURE SETS FOR ACTED AND SPONTANEOUS SPEECH IN VIEW OF AUTOMATIC EMOTION RECOGNITION.pdf:pdf},
title = {{COMPARING FEATURE SETS FOR ACTED AND SPONTANEOUS SPEECH IN VIEW OF AUTOMATIC EMOTION RECOGNITION}}
}
@article{Rao2013,
abstract = {In this paper, global and local prosodic features extracted from sentence, word and syllables are proposed for speech emotion or affect recognition. In this work, duration, pitch, and energy values are used to represent the prosodic information, for recognizing the emotions from speech. Global prosodic features represent the gross statistics such as mean, minimum, maximum, standard deviation, and slope of the prosodic contours. Local prosodic features represent the temporal dynamics in the prosody. In this work, global and local prosodic features are analyzed separately and in combination at different levels for the recognition of emotions. In this study, we have also explored the words and syllables at different positions (initial, middle, and ﬁnal) separately, to analyze their contribution towards the recognition of emotions. In this paper, all the studies are carried out using simulated Telugu emotion speech corpus (IITKGP-SESC). These results are compared with the results of internationally known Berlin emotion speech corpus (Emo-DB). Support vector machines are used to develop the emotion recognition models. The results indicate that, the recognition performance using local prosodic features is better compared to the performance of global prosodic features. Words in the ﬁnal position of the sentences, syllables in the ﬁnal position of the words exhibit more emotion discriminative information compared to the words and syllables present in the other positions.},
author = {Rao, K. Sreenivasa and Koolagudi, Shashidhar G. and Vempada, Ramu Reddy},
doi = {10.1007/s10772-012-9172-2},
isbn = {1381-2416},
issn = {13812416},
journal = {International Journal of Speech Technology},
number = {2},
title = {{Emotion recognition from speech using global and local prosodic features}},
volume = {16},
year = {2013}
}
@article{Bonora2011,
abstract = {Patients with chronic medial temporal lobe epilepsy (MTLE) can be impaired in different tasks that evaluate emotional or social abilities. In particular, the recognition of facial emotions can be affected (Meletti S, Benuzzi F, Rubboli G, et al. Neurology 2003;60:426-31. Meletti S, Benuzzi F, Cantalupo G, Rubboli G, Tassinari CA, Nichelli P. Epilepsia 2009;50:1547-59). To better understand the nature of emotion recognition deficits in MTLE we investigated the decoding of basic emotions in the visual (facial expression) and auditory (emotional prosody) domains in 41 patients. Results showed deficits in the recognition of both facial and vocal expression of emotions, with a strong correlation between performances across the two tasks. No correlation between emotion recognition and measures of IQ, quality of life (QOLIE-31), and depression (Beck Depression Inventory) was significant, except for a weak correlation between prosody recognition and IQ. These data suggest that emotion recognition impairment in MTLE is not dependent on the sensory channel through which the emotional stimulus is transmitted. Moreover, these findings support the notion that emotional processing is at least partly independent of measures of cognitive intelligence. {\textcopyright} 2011 Elsevier Inc.},
author = {Bonora, Annalisa and Benuzzi, Francesca and Monti, Giulia and Mirandola, Laura and Pugnaghi, Matteo and Nichelli, Paolo and Meletti, Stefano},
doi = {10.1016/j.yebeh.2011.01.027},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonora et al. - 2011 - Recognition of emotions from faces and voices in medial temporal lobe epilepsy.pdf:pdf},
isbn = {1525-5069 (Electronic)$\backslash$n1525-5050 (Linking)},
issn = {15255050},
journal = {Epilepsy and Behavior},
keywords = {Amygdala,Emotion,Emotional prosody,Facial emotion recognition,Facial expressions,Medial temporal lobe epilepsy},
pmid = {21459049},
title = {{Recognition of emotions from faces and voices in medial temporal lobe epilepsy}},
year = {2011}
}
@article{Anagnostopoulos2012,
abstract = {Speaker emotion recognition is achieved through processing methods that include isolation of the speech signal and extraction of selected features for the final classi- fication. In terms of acoustics, speech processing techniques offer extremely valuable para- linguistic information derived mainly from prosodic and spectral features. In some cases, the process is assisted by speech recognition systems,which contribute to the classification using linguistic information. Both frameworks deal with a very challenging problem, as emotional states do not have clear-cut boundaries and often differ from person to person. In this article, research papers that investigate emotion recognition from audio channels are surveyed and classified, basedmostly on extracted and selected features and their classificationmethodol- ogy. Important topics from different classification techniques, such as databases available for experimentation, appropriate feature extraction and selection methods, classifiers and per- formance issues are discussed, with emphasis on research published in the last decade. This survey also provides a discussion on open trends, along with directions for future research on this topic.},
author = {Anagnostopoulos, Christos Nikolaos and Iliou, Theodoros and Giannoukos, Ioannis},
doi = {10.1007/s10462-012-9368-5},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anagnostopoulos, Iliou, Giannoukos - 2012 - Features and classifiers for emotion recognition from speech a survey from 2000 to 2011.pdf:pdf},
issn = {15737462},
journal = {Artificial Intelligence Review},
keywords = {Classifiers,Emotion recognition,Speech features},
title = {{Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011}},
year = {2012}
}
@article{Chatterjee2014,
abstract = {Despite their remarkable success in bringing spoken language to hearing impaired listeners, the signal transmitted through cochlear implants (CIs) remains impoverished in spectro-temporal fine structure. As a consequence, pitch-dominant information such as voice emotion, is diminished. For young children, the ability to correctly identify the mood/intent of the speaker (which may not always be visible in their facial expression) is an important aspect of social and linguistic development. Previous work in the field has shown that children with cochlear implants (cCI) have significant deficits in voice emotion recognition relative to their normally hearing peers (cNH). Here, we report on voice emotion recognition by a cohort of 36 school-aged cCI. Additionally, we provide for the first time, a comparison of their performance to that of cNH and NH adults (aNH) listening to CI simulations of the same stimuli. We also provide comparisons to the performance of adult listeners with CIs (aCI), most of whom learned language primarily through normal acoustic hearing. Results indicate that, despite strong variability, on average, cCI perform similarly to their adult counterparts; that both groups' mean performance is similar to aNHs' performance with 8-channel noise-vocoded speech; that cNH achieve excellent scores in voice emotion recognition with full-spectrum speech, but on average, show significantly poorer scores than aNH with 8-channel noise-vocoded speech. A strong developmental effect was observed in the cNH with noise-vocoded speech in this task. These results point to the considerable benefit obtained by cochlear-implanted children from their devices, but also underscore the need for further research and development in this important and neglected area.This article is part of a Special Issue entitled {\textless}Lasker Award{\textgreater}.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Chatterjee, Monita and Zion, Danielle J. and Deroche, Mickael L. and Burianek, Brooke A. and Limb, Charles J. and Goren, Alison P. and Kulkarni, Aditya M. and Christensen, Julie A.},
doi = {10.1016/j.heares.2014.10.003},
eprint = {15334406},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatterjee et al. - 2014 - Voice emotion recognition by cochlear-implanted children and their normally-hearing peers.pdf:pdf},
isbn = {0378-5955},
issn = {18785891},
journal = {Hearing Research},
pmid = {25448167},
title = {{Voice emotion recognition by cochlear-implanted children and their normally-hearing peers}},
year = {2014}
}
@article{Palmer2005,
abstract = {There has been some debate recently over the scoring, reliability and factor structure of ability measures of emotional intelligence (EI). This study examined these three psychometric properties with the most recent ability test of EI, the Mayer - Salovey - Caruso Emotional Intelligence Test (MSCEIT V2.0; Mayer, Salovey, {\&} Caruso, [Mayer, J. D., Salovey, {\&} P., Caruso, (2000). Models of emotional intelligence. In R. J., Sternberg (Ed.). Handbook of intelligence (pp. 396-420). New York: Cambridge; Mayer, J. D., Salovey, P., {\&} Caruso, D. R., (2000). The Mayer, Salovey, and Caruso emotional intelligence test: Technical manual. Toronto, ON: MHS]), with a sample (n=431) drawn from the general population. The reliability of the MSCEIT at the total scale, area and branch levels was found to be good, although the reliability of most of the subscales was relatively low. Consistent with previous findings, there was a high level of convergence between the alternative scoring methods (consensus and expert). However, unlike Mayer et al.'s [Mayer, J. D., Salovey, P., Caruso, D. R., {\&} Sitarenios, G. (2003). Measuring emotional intelligence with the MSCEIT V2. 0. Emotion, 3, 97-105.] contentions, there was only partial support for their four-factor model of EI. A model with a general first-order factor of EI and a three first-order branch level factors was determined to be the best fitting model. There was no support for the Experiential Area level factor, nor was there support for the Facilitating Branch level factor. These results were replicated closely using the Mayer et al. [Mayer, J. D., Salovey, P., Caruso, D. R., {\&} Sitarenios, G., (2003). Measuring emotional intelligence with the MSCEIT V2. 0. Emotion, 3, 97-105.] data. The results are discussed in light of the close comparability of the two scoring methods. Furthermore, the fundamental limitations of the MSCEIT V2.0, with respect to the inadequate number of subscales theorized to measure each branch level factor are identified and discussed. ?? 2004 Elsevier Inc. All rights reserved.},
author = {Palmer, Benjamin R. and Gignac, Gilles and Manocha, Ramesh and Stough, Con},
doi = {10.1016/j.intell.2004.11.003},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Palmer et al. - 2005 - A psychometric evaluation of the Mayer-Salovey-Caruso Emotional Intelligence Test Version 2.0.pdf:pdf},
isbn = {Print 0160-2896 Elsevier Science Electronic},
issn = {01602896},
journal = {Intelligence},
keywords = {Emotional competencies,Emotional intelligence,Emotions,Factor structure,Reliability},
title = {{A psychometric evaluation of the Mayer-Salovey-Caruso Emotional Intelligence Test Version 2.0}},
year = {2005}
}
@article{Verma,
abstract = {—Recent times have been marked with the increasing demand for more intelligent human computer interfaces. By adding emotion recognition abilities, voice based interfaces can be made more human centric. As natural languages do not share similar acoustic-phonetic features and vary in production of speech sound, the emotion recognition accuracy gets affected with respect to the user's language. This work aims at studying the patterns of stress and intonation for emotional speech in Hindi (Indo-Aryan language) and analyzing the influence of gender on speech emotion recognition accuracy. The paper proposes a combined system for gender distinction and emotion recognition by extracting basic prosodic and spectral speech features and also compares three different classification algorithms. The performed experimentation over a Hindi emotional corpus reveals that 78{\%} correct speech emotion recognition accuracy can obtained by adopting support vector machines for classification.},
author = {Verma, Devika and Mukhopadhyay, Debajyoti and Mark, Emmanuel},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Verma, Mukhopadhyay, Mark - Unknown - Role of Gender Influence in Vocal Hindi Conversations A Study on Speech Emotion Recognition.pdf:pdf},
keywords = {gender recognition,human computer intelligent interaction,prosodic features,spectral features,support vector machine,—speech emotion recognition},
title = {{Role of Gender Influence in Vocal Hindi Conversations: A Study on Speech Emotion Recognition}}
}
@misc{Laback2015,
abstract = {Bilateral cochlear implantation is increasingly becoming the standard in the clinical treatment of bilateral deafness. The main motivation is to provide users of bilateral cochlear implants (CIs) access to binaural cues essential for localizing sound sources and understanding speech in environments of interfering sounds. One of those cues, interaural level differences, can be perceived well by CI users to allow some basic left versus right localization. However, interaural time differences (ITDs) which are important for localization of low-frequency sounds and spatial release from masking are not adequately represented by clinical envelope-based CI systems. Here, we first review the basic ITD sensitivity of CI users, particularly their dependence on stimulation parameters like stimulation rate and place, modulation rate, and envelope shape in single-electrode stimulation, as well as stimulation level, electrode spacing, and monaural across-electrode timing in multiple-electrode stimulation. Then, we discuss factors involved in ITD perception in electric hearing including the match between highly phase-locked electric auditory nerve response properties and binaural cell properties, the restricted stimulation of apical tonotopic pathways, channel interactions in multiple-electrode stimulation, and the onset age of binaural auditory input. Finally, we present clinically available CI stimulation strategies and experimental strategies aiming at improving listeners' access to ITD cues.This article is part of a Special Issue entitled {\textless}Lasker Award{\textgreater}.},
author = {Laback, Bernhard and Egger, Katharina and Majdak, Piotr},
booktitle = {Hearing Research},
doi = {10.1016/j.heares.2014.10.004},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laback, Egger, Majdak - 2015 - Perception and coding of interaural time differences with bilateral cochlear implants.pdf:pdf},
isbn = {0378-5955},
issn = {18785891},
pmid = {25456088},
title = {{Perception and coding of interaural time differences with bilateral cochlear implants}},
year = {2015}
}
@article{Nwe2003,
abstract = {In emotion classification of speech signals, the popular features employed are statistics of fundamental frequency, energy contour, duration of silence and voice quality. However, the performance of systems employing these features degrades substantially when more than two categories of emotion are to be classified. In this paper, a text independent method of emotion classification of speech is proposed. The proposed method makes use of short time log frequency power coefficients (LFPC) to represent the speech signals and a discrete hidden Markov model (HMM) as the classifier. The emotions are classified into six categories. The category labels used are, the archetypal emotions of Anger, Disgust, Fear, Joy, Sadness and Surprise. A database consisting of 60 emotional utterances, each from twelve speakers is constructed and used to train and test the proposed system. Performance of the LFPC feature parameters is compared with that of the linear prediction Cepstral coefficients (LPCC) and mel-frequency Cepstral coefficients (MFCC) feature parameters commonly used in speech recognition systems. Results show that the proposed system yields an average accuracy of 78{\%} and the best accuracy of 96{\%} in the classification of six emotions. This is beyond the 17{\%} chances by a random hit for a sample set of 6 categories. Results also reveal that LFPC is a better choice as feature parameters for emotion classification than the traditional feature parameters. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Nwe, Tin Lay and Foo, Say Wei and {De Silva}, Liyanage C.},
doi = {10.1016/S0167-6393(03)00099-2},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nwe, Foo, De Silva - 2003 - Speech emotion recognition using hidden Markov models.pdf:pdf},
isbn = {0167-6393},
issn = {01676393},
journal = {Speech Communication},
keywords = {Emotional speech,Hidden Markov model,Human communication,Log frequency power coefficients,Recognition of emotion},
title = {{Speech emotion recognition using hidden Markov models}},
year = {2003}
}
@misc{Strelnikov2015,
abstract = {In this article, we review the PET neuroimaging literature, which indicates peculiarities of brain networks involved in speech restoration after cochlear implantation. We consider data on implanted patients during stimulation as well as during resting state, which indicates basic long-term reorganisation of brain functional architecture. On the basis of our analysis of neuroimaging literature and considering our own studies, we indicate that auditory recovery in deaf patients after cochlear implantation partly relies on visual cues. The brain develops mechanisms of audio-visual integration as a strategy to achieve high levels of speech recognition. It turns out that this neuroimaging evidence is in line with behavioural findings of better audiovisual integration in these patients. Thus, strong visually and audio-visually based rehabilitation during the first months after cochlear implantation would significantly improve and fasten the functional recovery of speech intelligibility and other auditory functions in these patients. We provide perspectives for further neuroimaging studies in cochlear implanted patients, which would help understand brain organisation to restore auditory cognitive processing in the implanted patients and would potentially suggest novel approaches for their rehabilitation.This article is part of a Special Issue entitled {\textless}Lasker Award{\textgreater}.},
author = {Strelnikov, K. and Marx, M. and Lagleyre, S. and Fraysse, B. and Deguine, O. and Barone, P.},
booktitle = {Hearing Research},
doi = {10.1016/j.heares.2014.10.001},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Strelnikov et al. - 2015 - PET-imaging of brain plasticity after cochlear implantation.pdf:pdf},
isbn = {0378-5955},
issn = {18785891},
pmid = {25448166},
title = {{PET-imaging of brain plasticity after cochlear implantation}},
year = {2015}
}
@article{Steidl2009,
author = {Steidl, Stefan},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Steidl - 2009 - Automatic Classification of Emotion-Related User States in Spontaneous Children's Speech.pdf:pdf},
title = {{Automatic Classification of Emotion-Related User States in Spontaneous Children's Speech}},
year = {2009}
}
@article{Buss,
abstract = {Monaural envelope correlation perception concerns the ability of listeners to discriminate stimuli based on the degree of correlation between the temporal envelopes of two or more frequency-separated bands of noise [Richards, J. Acoust. Soc. Am. 82, 1621–1630 (1987)]. Previous work has examined this ability for relatively narrow bandwidths, generally 100 Hz or less. The present experi-ment explored a wide range of bandwidths, from 25 to 1600 Hz, which included bands narrower and wider than a critical bandwidth. Stimuli were pairs of noise bands separated by a 500-Hz-wide spec-tral gap centered on 2250 Hz. The magnitude spectra of the pair of comodulated bands were either identical or reflected around the midpoint of the band, and performance was assessed with and with-out a low-pass noise masker. Although discrimination was best for intermediate bandwidths, mean performance was above chance for all bandwidths tested. Data were similar for stimuli with identi-cal and reflected magnitude spectra, and for stimuli with and without the low-pass masker. The one exception was particularly good performance for intermediate-bandwidth stimuli with identical spectra, for which some listeners reported hearing a tonal cue. Results indicate that listeners are flexi-ble in selecting spectral regions upon which to base across-frequency comparisons.},
author = {Buss, Emily and {Hall Iii}, Joseph W and Grose, John H},
doi = {10.1121/1.4768887},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buss, Hall Iii, Grose - Unknown - Monaural envelope correlation perception for bands narrower or wider than a critical band(2).pdf:pdf},
pages = {405--416},
title = {{Monaural envelope correlation perception for bands narrower or wider than a critical band}}
}
@article{Wang2015,
abstract = {Recently, studies have been performed on harmony features for speech emotion recognition. It is found in our study that the first- and second-order differences of harmony features also play an important role in speech emotion recognition. Therefore, we propose a new Fourier parameter model using the perceptual content of voice quality and the first- and second-order differences for speaker-independent speech emotion recognition. Experimental results show that the proposed Fourier parameter (FP) features are effective in identifying various emotional states in speech signals. They improve the recognition rates over the methods using Mel frequency cepstral coefficient (MFCC) features by 16.2, 6.8 and 16.6 points on the German database (EMODB), Chinese language database (CASIA) and Chinese elderly emotion database (EESDB). In particular, when combining FP with MFCC, the recognition rates can be further improved on the aforementioned databases by 17.5, 10 and 10.5 points, respectively.},
author = {Wang, Kunxia and An, Ning and Li, Bing Nan and Zhang, Yanyong and Li, Lian},
doi = {10.1109/TAFFC.2015.2392101},
isbn = {1949-3045},
issn = {19493045},
journal = {IEEE Transactions on Affective Computing},
number = {1},
title = {{Speech emotion recognition using Fourier parameters}},
volume = {6},
year = {2015}
}
@article{KrishnaKishore2013,
abstract = {Recognition of emotions from speech is one of the most important sub domains in the field of affective computing. Six basic emotional states are considered for classification of emotions from speech in this work. In this work, features are extracted from audio characteristics of emotional speech by Mel-frequency Cepstral Coefficient (MFCC), and Subband based Cepstral Parameter (SBC) method. Further these features are classified using Gaussian Mixture Model (GMM). SAVEE audio database is used in this work for testing of Emotions. In the experimental results, SBC method out performs with 70{\%} in recognition compared to 51{\%} of recognition in MFCC algorithm.},
author = {{Krishna Kishore}, K. V. and {Krishna Satish}, P.},
doi = {10.1109/IAdCC.2013.6514336},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krishna Kishore, Krishna Satish - 2013 - Emotion recognition in speech using MFCC and wavelet features.pdf:pdf},
isbn = {9781467345286},
journal = {Proceedings of the 2013 3rd IEEE International Advance Computing Conference, IACC 2013},
keywords = {Gaussian Mixture Model (GMM),Mel-frequency Cepstral Coefficient (MFCC),Subband based Cepstral Parameter (SBC)},
pages = {842--847},
title = {{Emotion recognition in speech using MFCC and wavelet features}},
year = {2013}
}
@article{,
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Emotion Detection from Speech.pdf:pdf},
title = {{Emotion Detection from Speech}}
}
@article{Svirsky2015,
abstract = {What is the best way to help humans adapt to a distorted sensory input? Interest in this question is more than academic. The answer may help facilitate auditory learning by people who became deaf after learning language and later received a cochlear implant (a neural prosthesis that restores hearing through direct electrical stimulation of the auditory nerve). There is evidence that some cochlear implants (which provide information that is spectrally degraded to begin with) stimulate neurons with higher characteristic frequency than the acoustic frequency of the original stimulus. In other words, the stimulus is shifted in frequency with respect to what the listener expects to hear. This frequency misalignment may have a negative influence on speech perception by CI users. However, a perfect frequency-place alignment may result in the loss of important low frequency speech information. A trade-off may involve a gradual approach: start with correct frequency-place alignment to allow listeners to adapt to the spectrally degraded signal first, and then gradually increase the frequency shift to allow them to adapt to it over time. We used an acoustic model of a cochlear implant to measure adaptation to a frequency-shifted signal, using either the gradual approach or the "standard" approach (sudden imposition of the frequency shift). Listeners in both groups showed substantial auditory learning, as measured by increases in speech perception scores over the course of fifteen one-hour training sessions. However, the learning process was faster for listeners who were exposed to the gradual approach. These results suggest that gradual rather than sudden exposure may facilitate perceptual learning in the face of a spectrally degraded, frequency-shifted input.This article is part of a Special Issue entitled {\textless}Lasker Award{\textgreater}.},
author = {Svirsky, Mario A. and Talavage, Thomas M. and Sinha, Shivank and Neuburger, Heidi and Azadpour, Mahan},
doi = {10.1016/j.heares.2014.10.008},
file = {:home/wdk/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Svirsky et al. - 2015 - Gradual adaptation to auditory frequency mismatch.pdf:pdf},
isbn = {0378-5955},
issn = {18785891},
journal = {Hearing Research},
pmid = {25445816},
title = {{Gradual adaptation to auditory frequency mismatch}},
year = {2015}
}
@article{Boersma1993,
abstract = {We present a straightforward and robust algorithm for periodicity detection, working in the lag (autocorrelation) domain. When it is tested for periodic signals and for signals with additive noise or jitter, it proves to be several orders of magnitude more accurate than the methods commonly used for speech analysis. This makes our method capable of measuring harmonics-to-noise ratios in the lag domain with an accuracy and reliability much greater than that of any of the usual frequency-domain methods.},
author = {Boersma, Paul},
doi = {10.1371/journal.pone.0069107},
file = {:tmp/fundamentalf.pdf:pdf},
journal = {Proceedings of the Institute of Phonetic Sciences},
pages = {97--110},
title = {{Accurate Short-Term Analysis of the Fundamental Frequency and the Harmonics-To-Noise Ratio of a Sampled Sound}},
url = {http://www.cs.northwestern.edu/{~}pardo/courses/eecs352/papers/boersma-pitchtracking.pdf},
volume = {17},
year = {1993}
}
